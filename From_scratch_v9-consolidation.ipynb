{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ds = datasets.MNIST('data', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "test_ds = datasets.MNIST('data', train=False, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "class FlatMNIST(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.n = len(self.dataset)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx]\n",
    "        return x.view(28*28), y\n",
    "\n",
    "    def __len__(self): return self.n\n",
    "    \n",
    "tr_ds = FlatMNIST(train_ds)\n",
    "ts_ds = FlatMNIST(test_ds)\n",
    "\n",
    "batch_size = 64\n",
    "#batch_size = 5 # for testing\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "if cuda:\n",
    "    train_loader = torch.utils.data.DataLoader(tr_ds, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(ts_ds, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "else:\n",
    "    train_loader = torch.utils.data.DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(ts_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilty Tree\n",
    "\n",
    "Used for intermediate calculations in the tree structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class prob_tree:\n",
    "    \"\"\"\n",
    "    Recursive class,  Designed for tree calculations \n",
    "    up and down the different branches\n",
    "    \"\"\"\n",
    "    def __init__(self, list_prob, node_prob=None, path_prob=None, lvl=1, max_depth=2):\n",
    "        \"\"\"\n",
    "        Initializes the tree. Takes in a list of probabilities \n",
    "        corresponding to trees:\n",
    "        \n",
    "        listprob = [ root, l, r, ll, lr, rl, rr .....]\n",
    "        lvl = assumes the current level (increases with recursion)\n",
    "        max_depth = end condition\n",
    "        path_prob = probability to REACH the current node, if \n",
    "            first node, the probability is 100%\n",
    "        node_prob = the probability of the current node to the next split\n",
    "        \"\"\"\n",
    "        \n",
    "        # if there is no path prob, then 100%\n",
    "        # must be passed \n",
    "        if path_prob is None:\n",
    "            self.path_prob = Variable(torch.ones(list_prob[0].shape))\n",
    "        else:\n",
    "            self.path_prob = path_prob\n",
    "            \n",
    "        # this is the probability within the current node\n",
    "        # if node probability isn't passed, pull from list\n",
    "        if node_prob is None:\n",
    "            self.prob = list_prob.pop(0)\n",
    "        else:\n",
    "            self.prob = node_prob\n",
    "        \n",
    "        self.lvl = lvl\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        \n",
    "        # since we are doing breath first searching\n",
    "        # will pass the node probabilities forward\n",
    "        if lvl < max_depth:\n",
    "            left_node_prob = list_prob.pop(0)\n",
    "            right_node_prob = list_prob.pop(0)\n",
    "            \n",
    "            self.left = prob_tree(list_prob, left_node_prob, self.path_prob*self.prob, self.lvl+1, self.max_depth)\n",
    "            self.right = prob_tree(list_prob, right_node_prob, self.path_prob*(1-self.prob), self.lvl+1, self.max_depth)\n",
    "        else:\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            \n",
    "    def get_leaf_path_probs(self):\n",
    "        \"\"\"\n",
    "        returns a list of path probabilities for only the leaf (recursive)\n",
    "        \"\"\"\n",
    "        if self.left is None:\n",
    "            return([self.path_prob*self.prob, self.path_prob*(1-self.prob) ])\n",
    "        else:\n",
    "            return(self.left.get_leaf_path_probs() + self.right.get_leaf_path_probs())\n",
    "            \n",
    "    def get_inner_path_probs(self):\n",
    "        \"\"\"\n",
    "        returns a list of path probabilities for only the inner nodes (recursive)\n",
    "        uses breath first algorithm\n",
    "        \"\"\"        \n",
    "        path_probs = []\n",
    "        nodes = [self]\n",
    "        while nodes:\n",
    "            current = nodes.pop(0)\n",
    "            path_probs.append((self.lvl-1,current.path_prob))\n",
    "            if current.left:\n",
    "                nodes.append(current.left)\n",
    "                nodes.append(current.right)\n",
    "        return(path_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Tree Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NTree3(nn.Module):\n",
    "    def __init__(self, tree_depth=3, n_classes=10, ni=28*28, lmbda = 0.1, on_cuda=False, leaf_type ='const'):\n",
    "        super(NTree3,self).__init__()\n",
    "        self.n_leaves = 2**tree_depth\n",
    "        self.n_classes = n_classes\n",
    "        self.n_nodes = self.n_leaves -1\n",
    "        self.tree_depth = tree_depth\n",
    "        self.on_cuda = on_cuda\n",
    "        self.leaf_type = leaf_type\n",
    "        self.ptree = None\n",
    "        \n",
    "        # regularization\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "\n",
    "        leaf_params = [torch.randn(self.n_classes) for i in range(self.n_leaves)]\n",
    "        beta_params = [torch.rand(1) for i in range(self.n_nodes)]\n",
    "                \n",
    "        if self.on_cuda==True:\n",
    "            beta_params = [beta_param.cuda() for beta_param in beta_params]\n",
    "            leaf_params = [leaf_param.cuda() for leaf_param in leaf_params]\n",
    "        \n",
    "        self.nodes =  nn.ModuleList([nn.Linear(ni, 1) for i in range(self.n_nodes)])\n",
    "        \n",
    "        if self.leaf_type == 'const':\n",
    "            self.leaves = nn.ParameterList([nn.Parameter(leaf_param) for leaf_param in leaf_params])\n",
    "        elif self.leaf_type == 'logreg':\n",
    "            self.leaves = nn.ModuleList([nn.Linear(ni, n_classes) for i in range(self.n_leaves)])\n",
    "                        \n",
    "        # inverse temperature filter\n",
    "        self.betas = nn.ParameterList([nn.Parameter(beta) for beta in beta_params])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        bz = x.size()[0]\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # if we assume constant distribution in the leaft nodes\n",
    "        if self.leaf_type == 'const': \n",
    "            softmax = nn.Softmax(dim=0)\n",
    "        else: \n",
    "            softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        \n",
    "        # create distributions at each leaf - store for later\n",
    "        if self.leaf_type == 'const':\n",
    "            leaf_pcts = [softmax(leaf_param) for leaf_param in self.leaves]\n",
    "            leaf_dist = [pct.expand(bz, self.n_classes) for pct in leaf_pcts]\n",
    "            self.leaf_pcts = leaf_pcts\n",
    "        else:\n",
    "            leaf_dist = [softmax(leaf(x)) for leaf in self.leaves]\n",
    "        \n",
    "        \n",
    "        # probabilities of inner nodes\n",
    "        tmp = [self.nodes[i](x) for i in range(self.n_nodes)]\n",
    "        \n",
    "        p_x = [sigmoid(self.betas[i]*self.nodes[i](x)) for i in range(self.n_nodes)]\n",
    "        \n",
    "        tmp = [x for x in p_x]\n",
    "        pt = prob_tree(tmp, max_depth=self.tree_depth)\n",
    "        \n",
    "        path_prob = pt.get_leaf_path_probs()\n",
    "        \n",
    "        return leaf_dist, path_prob, p_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvTree(nn.Module):\n",
    "    def __init__(self, tree_depth=3, n_classes=10, ni=28*28, lmbda = 0.1, on_cuda=False, leaf_type ='const'):\n",
    "        super(ConvTree,self).__init__()\n",
    "        self.n_leaves = 2**tree_depth\n",
    "        self.n_classes = n_classes\n",
    "        self.n_nodes = self.n_leaves -1\n",
    "        self.tree_depth = tree_depth\n",
    "        self.on_cuda = on_cuda\n",
    "        self.leaf_type = leaf_type\n",
    "        self.ptree = None\n",
    "        \n",
    "        # regularization\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "\n",
    "        leaf_params = [torch.randn(self.n_classes) for i in range(self.n_leaves)]\n",
    "        beta_params = [torch.rand(1) for i in range(self.n_nodes)]\n",
    "                \n",
    "        if self.on_cuda==True:\n",
    "            beta_params = [beta_param.cuda() for beta_param in beta_params]\n",
    "            leaf_params = [leaf_param.cuda() for leaf_param in leaf_params]\n",
    "        \n",
    "        self.conv_nodes = nn.ModuleList([nn.Conv2d(1,1,11) for i in range(self.n_nodes)])\n",
    "        self.nodes =  nn.ModuleList([nn.Linear(18*18, 1) for i in range(self.n_nodes)])\n",
    "        \n",
    "        if self.leaf_type == 'const':\n",
    "            self.leaves = nn.ParameterList([nn.Parameter(leaf_param) for leaf_param in leaf_params])\n",
    "        elif self.leaf_type == 'logreg':\n",
    "            self.conv_leaves = nn.ModuleList([nn.Conv2d(1,1,11) for i in range(self.n_leaves)])\n",
    "            self.leaves = nn.ModuleList([nn.Linear(18*18, n_classes) for i in range(self.n_leaves)])\n",
    "                        \n",
    "        # inverse temperature filter\n",
    "        self.betas = nn.ParameterList([nn.Parameter(beta) for beta in beta_params])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        bz = x.size()[0]\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # if we assume constant distribution in the leaft nodes\n",
    "        if self.leaf_type == 'const': \n",
    "            softmax = nn.Softmax(dim=0)\n",
    "        else: \n",
    "            softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        \n",
    "        # create distributions at each leaf - store for later\n",
    "        if self.leaf_type == 'const':\n",
    "            leaf_pcts = [softmax(leaf_param) for leaf_param in self.leaves]\n",
    "            leaf_dist = [pct.expand(bz, self.n_classes) for pct in leaf_pcts]\n",
    "            self.leaf_pcts = leaf_pcts\n",
    "        else:\n",
    "            conv_calc = [self.conv_leaves[i](x) for i in range(self.n_leaves)]\n",
    "            conv_calc2 = [conv_out.view(bz,-1) for conv_out in conv_calc]\n",
    "            leaf_dist = [softmax(leaf(conv_calc2[idx])) for idx, leaf in enumerate(self.leaves)]\n",
    "        \n",
    "        \n",
    "        # probabilities of inner nodes\n",
    "        conv_calc = [self.conv_nodes[i](x) for i in range(self.n_nodes)]\n",
    "        conv_calc2 = [conv_out.view(bz,-1) for conv_out in conv_calc]\n",
    "        p_x = [sigmoid(self.betas[i]*self.nodes[i](conv_calc2[i])) for i in range(self.n_nodes)]\n",
    "        \n",
    "        tmp = [x for x in p_x]\n",
    "        pt = prob_tree(tmp, max_depth=self.tree_depth)\n",
    "        \n",
    "        path_prob = pt.get_leaf_path_probs()\n",
    "        \n",
    "        return leaf_dist, path_prob, p_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigot_leaf_loss(path_prob, leaf_dist, labels, on_cuda):\n",
    "    ymask = torch.FloatTensor(leaf_dist.size()).zero_()\n",
    "    if on_cuda:\n",
    "        ymask = ymask.cuda()    \n",
    "    ymask.scatter_(1, labels.data.view(-1,1), 1)\n",
    "    ymask = Variable(ymask)\n",
    "    Tk_log_Qk = (torch.log(leaf_dist) * ymask).sum(1)\n",
    "    PTQ = Tk_log_Qk[:,None]*path_prob\n",
    "    return torch.sum(PTQ)\n",
    "\n",
    "\n",
    "def penalty(p_x, max_depth, lmbda=0.1):\n",
    "    \n",
    "    tmp = [x for x in p_x]\n",
    "    ptree = prob_tree(tmp, max_depth=max_depth)\n",
    "    P_x = ptree.get_inner_path_probs()\n",
    "    \n",
    "    alphas = [(Px[0], torch.sum(torch.mul(Px[1],px))/torch.sum(Px[1])) for px, Px in zip(p_x, P_x)]\n",
    "    \n",
    "    def c1(alpha_i):\n",
    "        return 0.5*np.log(alpha_i) + 0.5*np.log(1-alpha_i)\n",
    "\n",
    "    C =  np.sum([-lmbda*2**-depth*c1(alp) for depth, alp in alphas])\n",
    "    return C\n",
    "\n",
    "def total_loss(path_probs, leaf_dists, p_x, labels, max_depth, lmbda=0.1,  on_cuda=False):\n",
    "    L_x = [bigot_leaf_loss(path_prob, leaf_dist, labels, on_cuda) for path_prob, leaf_dist in zip(path_probs, leaf_dists)]\n",
    "    C = penalty(p_x, max_depth, lmbda)\n",
    "    return(torch.log(-torch.sum(torch.stack(L_x)))+C)\n",
    "\n",
    "\n",
    "def which_node(path_prob, n_leaves, on_cuda=False):\n",
    "    node_id = torch.max(torch.stack(path_prob),dim=0)[1]    \n",
    "    nodes_onehot = torch.FloatTensor(path_prob[0].size()[0], n_leaves).zero_()\n",
    "    if on_cuda:\n",
    "        node_id = node_id.cuda()\n",
    "        nodes_onehot = nodes_onehot.cuda()\n",
    "    node_mask = nodes_onehot.scatter_(1, node_id.data,1)\n",
    "    return(node_id,node_mask)\n",
    "\n",
    "\n",
    "def which_class(path_prob, leaf_dist, on_cuda=False):\n",
    "    n_leaves = len(leaf_dist)\n",
    "    node_id, node_mask = which_node(path_prob, n_leaves, on_cuda)\n",
    "    max_class_per_node = torch.t(torch.max(torch.stack(leaf_dist),dim=2)[1])\n",
    "    pred_class = torch.sum(Variable(node_mask.long())*max_class_per_node,dim=1)\n",
    "    return(pred_class)\n",
    "\n",
    "\n",
    "def acc_calc(val_dl, model, on_cuda=False):\n",
    "    model.eval()\n",
    "    val_ = iter(val_dl)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    final_dist = 0\n",
    "\n",
    "    for i, batch in enumerate(val_dl):\n",
    "        data, labels = batch\n",
    "        if on_cuda:\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "        data_var, labels_var = Variable(data), Variable(labels)        \n",
    "        leaf_dist, path_prob, p_nodes = model(data_var)\n",
    "        final_dist += which_node(path_prob, n_leaves = model.n_leaves, on_cuda=on_cuda)[1].sum(0)\n",
    "        \n",
    "        preds = which_class(path_prob, leaf_dist, on_cuda=on_cuda)\n",
    "        match = labels.eq(preds.data)\n",
    "        correct += match.sum()\n",
    "        total += match.size()[0]\n",
    "    return(correct/total, correct, total, final_dist) \n",
    "\n",
    "def plt_node_dist(leaf_pcts):\n",
    "    \"\"\"\n",
    "    .store_preds() must be run by calling .predict()\n",
    "    \"\"\"\n",
    "    n_leaves = len(leaf_pcts)\n",
    "    n_half = int(n_leaves/2) \n",
    "    fig, ax_array = plt.subplots(2, n_half, sharey=True)\n",
    "    fig.set_figwidth(12)\n",
    "    plt.setp(ax_array, xticks=[0,1,2,3,4,5,6,7,8,9])\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    for i, leaf_pct in enumerate(leaf_pcts):\n",
    "        pct_dict = {k:v for k,v in enumerate(leaf_pct.data.numpy())}\n",
    "        x = list(pct_dict.keys())\n",
    "        y = list(pct_dict.values())\n",
    "        if i < n_half:\n",
    "            ax_array[0,i].bar(x, height=y)\n",
    "            ax_array[0,i].set_title('Node %d' % i)\n",
    "        else:\n",
    "            ax_array[1,i-n_half].bar(x, height=y)\n",
    "            ax_array[1,i-n_half].set_title('Node %d' % i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_idx(batch_prob, master_tensor):\n",
    "    EMPTY_TENSOR = torch.zeros(1).nonzero()\n",
    "    local_prob = batch_prob.index_select(dim=0, index=Variable(master_tensor[:,0].long()))\n",
    "    left_mask = local_prob > 0.5\n",
    "    right_mask = local_prob <= 0.5\n",
    "    \n",
    "    l = left_mask.nonzero()\n",
    "    r = right_mask.nonzero()\n",
    "    \n",
    "    if len(l)>0:    \n",
    "        left_mask_idx = l[:,0].data    \n",
    "        left_idx = master_tensor.index_select(dim=0,index=left_mask_idx)\n",
    "    else:\n",
    "        left_idx = EMPTY_TENSOR\n",
    "        \n",
    "    if len(r)>0:\n",
    "        right_mask_idx = r[:,0].data\n",
    "        right_idx = master_tensor.index_select(dim=0,index=right_mask_idx)\n",
    "    else:\n",
    "        right_idx = EMPTY_TENSOR\n",
    "    return(left_idx, right_idx)\n",
    "\n",
    "\n",
    "def distribute_points(labels_var, p_x, max_depth):\n",
    "    EMPTY_TENSOR = torch.zeros(1).nonzero()\n",
    "    master_list = [(i,v) for i, v in enumerate(labels_var.data)]\n",
    "    master_tensor = torch.Tensor(master_list)\n",
    "\n",
    "    output = [master_tensor]\n",
    "    idx_queue = [master_tensor]\n",
    "    tmp = [a for a in p_x]\n",
    "    \n",
    "    iter_limit = 2**(max_depth)\n",
    "    counter = 1\n",
    "    while counter < iter_limit:\n",
    "        counter +=1\n",
    "        current_prob = tmp.pop(0)\n",
    "        current_labels = idx_queue.pop(0)\n",
    "\n",
    "        left_labels, right_labels = split_idx(current_prob, current_labels)\n",
    "        if len(left_labels) > 0:\n",
    "            output.append(left_labels)\n",
    "            idx_queue.append(left_labels)\n",
    "        else:\n",
    "            output.append(EMPTY_TENSOR)\n",
    "\n",
    "        if len(right_labels) > 0 :\n",
    "            output.append(right_labels)\n",
    "            idx_queue.append(right_labels)\n",
    "        else:\n",
    "            output.append(EMPTY_TENSOR)\n",
    "    return(output)\n",
    "\n",
    "def add_index(tensor, offset):\n",
    "    tensor[:,0] = tensor[:,0] + offset\n",
    "    return(tensor)\n",
    "\n",
    "def spread_test_distr(bz, model, test_loader):\n",
    "    max_depth = model.tree_depth\n",
    "    test_loader = iter(test_loader)\n",
    "    master_points_dist = {i:[] for i in range(2**(max_depth+1)-1)}\n",
    "    \n",
    "    for i, batch in enumerate(test_loader):\n",
    "        img_data, labels = batch\n",
    "        img_data_var, labels_var = Variable(img_data), Variable(labels)\n",
    "        leaf_dist, path_prob, p_x = model(img_data_var)\n",
    "        \n",
    "        # initialize\n",
    "        \n",
    "        points_dist = distribute_points(labels_var, p_x, max_depth)\n",
    "        for i, tensor in enumerate(points_dist):\n",
    "            master_points_dist[i].append(tensor)\n",
    "            \n",
    "            \n",
    "    \n",
    "    for k, v in master_points_dist.items():\n",
    "        master_points_dist[k] = torch.cat([add_index(tensor, j*bz) for j, tensor in enumerate(v) if len(tensor) > 0],dim=0)\n",
    "\n",
    "    return master_points_dist\n",
    "\n",
    "def plot_test_tree_spread(bz, model, test_loader):\n",
    "    max_depth = model.tree_depth\n",
    "    res = spread_test_distr(bz, model, test_loader)\n",
    "    tmp = [a for a in res.values()]\n",
    "\n",
    "\n",
    "    for i in range(0,max_depth+1):\n",
    "        fig, ax_array = plt.subplots(1, 2**i, sharey=True)\n",
    "        fig.set_size_inches(14,4)\n",
    "        for j in range(2**i):\n",
    "            tensor = tmp.pop(0)\n",
    "            ctr = dict(Counter(tensor[:,1]))\n",
    "            x = list(ctr.keys())\n",
    "            y = list(ctr.values())\n",
    "            if i == 0:\n",
    "                ax_array.bar(x, height=y)\n",
    "            else:\n",
    "                ax_array[j].bar(x, height=y)\n",
    "                \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for a log model at leafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0 , 00000/60000,  L: 5.559, A: 0.1372, dist [635, 3248, 372, 1193, 852, 1533, 1888, 279]\n",
      "Ep: 0 , 12800/60000,  L: 3.372, A: 0.9278, dist [466, 769, 695, 838, 1345, 1296, 2356, 2235]\n",
      "Ep: 0 , 25600/60000,  L: 4.129, A: 0.9384, dist [700, 587, 794, 919, 1254, 1273, 2831, 1642]\n",
      "Ep: 0 , 38400/60000,  L: 2.912, A: 0.9453, dist [776, 1172, 908, 838, 1460, 1271, 1903, 1672]\n",
      "Ep: 0 , 51200/60000,  L: 2.772, A: 0.9458, dist [1046, 882, 846, 794, 1231, 1467, 2263, 1471]\n",
      "Ep: 1 , 00000/60000,  L: 3.270, A: 0.9437, dist [1184, 1339, 837, 787, 879, 1394, 2258, 1322]\n",
      "Ep: 1 , 12800/60000,  L: 3.105, A: 0.9465, dist [852, 1008, 851, 886, 1829, 1368, 1610, 1596]\n",
      "Ep: 1 , 25600/60000,  L: 1.320, A: 0.9510, dist [1132, 966, 707, 745, 1498, 1444, 1844, 1664]\n",
      "Ep: 1 , 38400/60000,  L: 3.431, A: 0.9486, dist [809, 1319, 808, 822, 1214, 1468, 1566, 1994]\n",
      "Ep: 1 , 51200/60000,  L: 3.045, A: 0.9516, dist [976, 1379, 805, 834, 1305, 1112, 1634, 1955]\n",
      "Ep: 2 , 00000/60000,  L: 1.831, A: 0.9521, dist [1013, 1126, 827, 751, 1534, 1289, 1923, 1537]\n",
      "Ep: 2 , 12800/60000,  L: 2.871, A: 0.9517, dist [886, 1012, 710, 798, 1634, 1370, 1732, 1858]\n",
      "Ep: 2 , 25600/60000,  L: 1.373, A: 0.9528, dist [901, 1130, 708, 808, 1399, 1492, 2086, 1476]\n",
      "Ep: 2 , 38400/60000,  L: 2.382, A: 0.9550, dist [858, 1280, 541, 778, 1512, 1515, 1723, 1793]\n",
      "Ep: 2 , 51200/60000,  L: 3.909, A: 0.9492, dist [863, 1292, 850, 767, 1591, 1403, 1420, 1814]\n",
      "Ep: 3 , 00000/60000,  L: 1.977, A: 0.9525, dist [739, 1694, 936, 841, 1459, 1216, 1637, 1478]\n",
      "Ep: 3 , 12800/60000,  L: 3.276, A: 0.9519, dist [816, 1241, 760, 808, 1553, 1416, 1798, 1608]\n",
      "Ep: 3 , 25600/60000,  L: 3.321, A: 0.9528, dist [939, 1299, 811, 944, 1459, 1377, 1366, 1805]\n",
      "Ep: 3 , 38400/60000,  L: 1.718, A: 0.9539, dist [999, 1184, 794, 906, 1621, 1376, 1288, 1832]\n",
      "Ep: 3 , 51200/60000,  L: 1.078, A: 0.9544, dist [1016, 1200, 791, 866, 1348, 1257, 1980, 1542]\n",
      "Ep: 4 , 00000/60000,  L: 1.262, A: 0.9517, dist [1004, 1315, 781, 828, 1154, 1456, 1301, 2161]\n",
      "Ep: 4 , 12800/60000,  L: 1.161, A: 0.9532, dist [862, 925, 842, 883, 1329, 1625, 1774, 1760]\n",
      "Ep: 4 , 25600/60000,  L: 1.862, A: 0.9528, dist [1024, 1510, 778, 950, 1128, 1243, 1547, 1820]\n",
      "Ep: 4 , 38400/60000,  L: 2.837, A: 0.9545, dist [975, 1377, 744, 845, 1284, 1236, 1699, 1840]\n",
      "Ep: 4 , 51200/60000,  L: 2.201, A: 0.9559, dist [796, 1342, 795, 925, 1366, 1326, 1830, 1620]\n",
      "Ep: 5 , 00000/60000,  L: 3.363, A: 0.9512, dist [706, 1187, 773, 882, 1520, 1392, 1791, 1749]\n",
      "Ep: 5 , 12800/60000,  L: 3.165, A: 0.9532, dist [1080, 1435, 931, 893, 1257, 1325, 1589, 1490]\n",
      "Ep: 5 , 25600/60000,  L: 2.008, A: 0.9536, dist [893, 1178, 905, 943, 1273, 1302, 1629, 1877]\n",
      "Ep: 5 , 38400/60000,  L: 2.063, A: 0.9552, dist [745, 1336, 860, 955, 1449, 1304, 1782, 1569]\n",
      "Ep: 5 , 51200/60000,  L: 1.989, A: 0.9534, dist [900, 1338, 850, 938, 1511, 1288, 1678, 1497]\n",
      "Ep: 6 , 00000/60000,  L: 1.934, A: 0.9543, dist [1024, 1220, 659, 892, 1579, 1349, 1517, 1760]\n",
      "Ep: 6 , 12800/60000,  L: 1.331, A: 0.9537, dist [1039, 1530, 746, 1018, 1297, 1255, 1434, 1681]\n",
      "Ep: 6 , 25600/60000,  L: 2.191, A: 0.9542, dist [954, 866, 772, 942, 1583, 1598, 1488, 1797]\n",
      "Ep: 6 , 38400/60000,  L: 2.924, A: 0.9557, dist [895, 1157, 834, 1042, 1165, 1376, 1850, 1681]\n",
      "Ep: 6 , 51200/60000,  L: 3.610, A: 0.9534, dist [934, 1130, 928, 883, 1450, 1205, 1858, 1612]\n",
      "Ep: 7 , 00000/60000,  L: 1.043, A: 0.9541, dist [1139, 1556, 839, 896, 1385, 1292, 1388, 1505]\n",
      "Ep: 7 , 12800/60000,  L: 3.162, A: 0.9535, dist [795, 1179, 895, 820, 1729, 1575, 1297, 1710]\n",
      "Ep: 7 , 25600/60000,  L: 2.896, A: 0.9547, dist [1067, 1393, 810, 844, 1375, 1279, 1718, 1514]\n",
      "Ep: 7 , 38400/60000,  L: 2.202, A: 0.9572, dist [941, 1499, 840, 865, 1287, 1325, 1611, 1632]\n",
      "Ep: 7 , 51200/60000,  L: 2.407, A: 0.9562, dist [934, 1167, 870, 885, 1548, 1202, 1677, 1717]\n",
      "Ep: 8 , 00000/60000,  L: 2.519, A: 0.9583, dist [963, 1169, 931, 922, 1391, 1247, 1926, 1451]\n",
      "Ep: 8 , 12800/60000,  L: 1.822, A: 0.9537, dist [848, 1236, 867, 970, 1457, 1420, 1576, 1626]\n",
      "Ep: 8 , 25600/60000,  L: 0.910, A: 0.9560, dist [942, 1005, 915, 900, 1765, 1385, 1412, 1676]\n",
      "Ep: 8 , 38400/60000,  L: 0.705, A: 0.9568, dist [998, 1673, 818, 927, 1213, 1109, 1556, 1706]\n",
      "Ep: 8 , 51200/60000,  L: 2.323, A: 0.9561, dist [954, 1276, 905, 871, 1348, 1445, 1617, 1584]\n",
      "Ep: 9 , 00000/60000,  L: 3.273, A: 0.9560, dist [950, 1298, 856, 935, 1510, 1346, 1438, 1667]\n",
      "Ep: 9 , 12800/60000,  L: 2.496, A: 0.9575, dist [935, 1297, 743, 801, 1558, 1406, 1488, 1772]\n",
      "Ep: 9 , 25600/60000,  L: 0.936, A: 0.9569, dist [977, 1167, 863, 927, 1409, 1219, 1584, 1854]\n",
      "Ep: 9 , 38400/60000,  L: -0.278, A: 0.9576, dist [844, 1510, 805, 870, 1319, 1280, 1553, 1819]\n",
      "Ep: 9 , 51200/60000,  L: 3.279, A: 0.9556, dist [848, 1343, 856, 862, 1630, 1302, 1512, 1647]\n"
     ]
    }
   ],
   "source": [
    "is_cuda = False\n",
    "lmbda = 0.1\n",
    "max_depth = 2\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "# flattened\n",
    "if is_cuda:\n",
    "    train_loader = torch.utils.data.DataLoader(tr_ds, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(ts_ds, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "else:\n",
    "    train_loader = torch.utils.data.DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(ts_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "model = NTree3(tree_depth=max_depth, n_classes=10, ni=28*28, lmbda = lmbda, on_cuda=is_cuda, leaf_type='logreg')\n",
    "\n",
    "if is_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "n_epochs = 5\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_dl = iter(train_loader)\n",
    "final_dist = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    train_dl = iter(train_loader)\n",
    "    \n",
    "    for i, batch in enumerate(train_dl):\n",
    "        data, labels = batch\n",
    "        bz = data.size()[0]\n",
    "        if is_cuda:\n",
    "            data = data.cuda()\n",
    "            labels = labels.cuda()\n",
    "        data_var, labels_var = Variable(data), Variable(labels)\n",
    "       \n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        leaf_dist, path_prob, p_x = model(data_var)\n",
    "        \n",
    "        loss = total_loss(path_prob, leaf_dist, p_x, labels_var, max_depth, lmbda, is_cuda)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%200 == 0:\n",
    "            acc, correct, total, dist = acc_calc(test_loader, model, on_cuda=is_cuda)\n",
    "            print('Ep: %d , %05d/60000,  L: %.03f, A: %.04f, dist %s' % (epoch, i*bz ,loss.data[0], acc, list(dist.long().cpu().numpy())))\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv Nodes, Log Leafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0 , 00000/60000,  L: 5.503, A: 0.1786, dist [1411, 2026, 1030, 2591, 874, 1310, 407, 351]\n",
      "Ep: 0 , 12800/60000,  L: 3.660, A: 0.9104, dist [1753, 1103, 1055, 1226, 1561, 1399, 994, 909]\n",
      "Ep: 0 , 25600/60000,  L: 3.377, A: 0.9259, dist [2100, 1345, 1301, 1221, 1229, 1026, 827, 951]\n",
      "Ep: 0 , 38400/60000,  L: 3.639, A: 0.9253, dist [1687, 1086, 1336, 952, 1284, 1301, 1391, 963]\n",
      "Ep: 0 , 51200/60000,  L: 2.218, A: 0.9373, dist [2376, 924, 1626, 1069, 1084, 847, 1143, 931]\n",
      "Ep: 1 , 00000/60000,  L: 3.176, A: 0.9351, dist [1709, 1394, 1219, 1236, 1423, 1289, 877, 853]\n",
      "Ep: 1 , 12800/60000,  L: 1.981, A: 0.9364, dist [1907, 1039, 1744, 1301, 1039, 946, 1028, 996]\n",
      "Ep: 1 , 25600/60000,  L: 3.696, A: 0.9332, dist [2068, 703, 1498, 1098, 1256, 1173, 1012, 1192]\n",
      "Ep: 1 , 38400/60000,  L: 3.277, A: 0.9429, dist [1324, 1028, 1540, 1012, 1512, 951, 1245, 1388]\n",
      "Ep: 1 , 51200/60000,  L: 3.551, A: 0.9381, dist [1425, 1187, 1560, 1193, 1225, 767, 1388, 1255]\n",
      "Ep: 2 , 00000/60000,  L: 2.093, A: 0.9452, dist [1523, 790, 1589, 1239, 1203, 1175, 1097, 1384]\n",
      "Ep: 2 , 12800/60000,  L: 3.473, A: 0.9472, dist [1747, 1286, 1749, 1311, 970, 1060, 785, 1092]\n",
      "Ep: 2 , 25600/60000,  L: 2.909, A: 0.9400, dist [1420, 1121, 1568, 1425, 1326, 813, 1114, 1213]\n",
      "Ep: 2 , 38400/60000,  L: 2.466, A: 0.9445, dist [1340, 1182, 1495, 1505, 1437, 851, 1178, 1012]\n",
      "Ep: 2 , 51200/60000,  L: 2.547, A: 0.9408, dist [1591, 1033, 1592, 1332, 1309, 881, 1330, 932]\n",
      "Ep: 3 , 00000/60000,  L: 3.420, A: 0.9492, dist [1924, 839, 1708, 1478, 1086, 964, 1076, 925]\n",
      "Ep: 3 , 12800/60000,  L: 3.833, A: 0.9490, dist [1676, 914, 1736, 1405, 1256, 893, 1116, 1004]\n",
      "Ep: 3 , 25600/60000,  L: 4.245, A: 0.9437, dist [1468, 1206, 2201, 1214, 1149, 912, 973, 877]\n",
      "Ep: 3 , 38400/60000,  L: 2.398, A: 0.9433, dist [1995, 1202, 1946, 1225, 804, 835, 1028, 965]\n",
      "Ep: 3 , 51200/60000,  L: 1.891, A: 0.9477, dist [1461, 1262, 1848, 1381, 1070, 916, 1169, 893]\n",
      "Ep: 4 , 00000/60000,  L: 1.840, A: 0.9461, dist [1450, 960, 1805, 1584, 1097, 857, 1205, 1042]\n",
      "Ep: 4 , 12800/60000,  L: 2.624, A: 0.9489, dist [1759, 1168, 1848, 1456, 1047, 947, 913, 862]\n",
      "Ep: 4 , 25600/60000,  L: 2.694, A: 0.9448, dist [1807, 864, 1843, 1364, 1377, 884, 968, 893]\n",
      "Ep: 4 , 38400/60000,  L: 2.125, A: 0.9450, dist [1986, 914, 1626, 1054, 1587, 708, 1066, 1059]\n",
      "Ep: 4 , 51200/60000,  L: 2.778, A: 0.9506, dist [1576, 809, 2236, 1429, 1288, 751, 1009, 902]\n"
     ]
    }
   ],
   "source": [
    "is_cuda = False\n",
    "lmbda = 0.1\n",
    "max_depth = 3\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "# flattened\n",
    "if is_cuda:\n",
    "    train_loader = torch.utils.data.DataLoader(tr_ds, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(ts_ds, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "else:\n",
    "    train_loader = torch.utils.data.DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(ts_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "# not flattened\n",
    "if is_cuda:\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "else:\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    \n",
    "model = ConvTree(tree_depth=max_depth, n_classes=10, ni=28*28, lmbda = lmbda, on_cuda=is_cuda, leaf_type='logreg')\n",
    "\n",
    "if is_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "n_epochs = 5\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_dl = iter(train_loader)\n",
    "final_dist = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    train_dl = iter(train_loader)\n",
    "    \n",
    "    for i, batch in enumerate(train_dl):\n",
    "        data, labels = batch\n",
    "        bz = data.size()[0]\n",
    "        if is_cuda:\n",
    "            data = data.cuda()\n",
    "            labels = labels.cuda()\n",
    "        data_var, labels_var = Variable(data), Variable(labels)\n",
    "       \n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        leaf_dist, path_prob, p_x = model(data_var)\n",
    "        \n",
    "        loss = total_loss(path_prob, leaf_dist, p_x, labels_var, max_depth, lmbda, is_cuda)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%200 == 0:\n",
    "            acc, correct, total, dist = acc_calc(test_loader, model, on_cuda=is_cuda)\n",
    "            print('Ep: %d , %05d/60000,  L: %.03f, A: %.04f, dist %s' % (epoch, i*bz ,loss.data[0], acc, list(dist.long().cpu().numpy())))\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing constant distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt_node_dist(model.leaf_pcts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAz8AAAD8CAYAAABZwRrEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118459358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAz8AAAD8CAYAAABZwRrEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAEedJREFUeJzt3W+MZXdZB/DvY8eKxWALHQjuFqeG\nBm1MDM2mFkmMoUYoNS4vaCwqbEjNvqmIYiILbzD+iWti5E80NQ2tFiUgKSRtbCM2BWJ8QcOWEqBU\n002t3bGVrilUIzHY+PjinqHT7ba7O3f23hl+n0+yuff87u/e88zpbJ/57vmdM9XdAQAA+G73Pcsu\nAAAAYBGEHwAAYAjCDwAAMAThBwAAGILwAwAADEH4AQAAhiD8AAAAQxB+AACAIQg/AADAEFaWXcDz\nufDCC3ttbW3ZZQAM7d577/2P7l5ddh07kT4FsDOcbq/a0eFnbW0tR44cWXYZAEOrqn9ddg07lT4F\nsDOcbq+y7A0AABiC8AMAAAxB+AEAAIYg/AAAAEMQfgAAgCEIPwAAwBCEHwAAYAjCDwAAMAThBwAA\nGMLKsgv4brd26I6F7evhw1cvbF8AALDbOPMDAAAMQfgBAACGIPwAAABDEH4AAIAhCD8AAMAQhB8A\nAGAIwg8AADAE4QcAABiC8AMAAAxB+AEAAIYg/AAAAEMQfgAAgCEIPwAAwBCEHwAAYAjCDwAAMATh\nBwAAGMIpw09V3VxVj1fVVzeNvbiq7qqqB6fHC6bxqqoPVdXRqvpyVV226T0HpvkPVtWBs/PlAAAA\nnNzpnPn5yyRvOGHsUJK7u/uSJHdP20lyVZJLpj8Hk9yQzMJSkvcl+ckklyd530ZgAgAAWIRThp/u\n/ockT5wwvD/JLdPzW5K8adP4R3rm80nOr6qXJ3l9kru6+4nu/kaSu/LsQAUAAHDWbPWan5d192NJ\nMj2+dBrfk+TYpnnr09hzjQMAACzEdt/woE4y1s8z/uwPqDpYVUeq6sjx48e3tTgAmJc+BbB7bTX8\nfH1azpbp8fFpfD3JRZvm7U3y6POMP0t339jd+7p73+rq6hbLA4CzQ58C2L22Gn5uT7Jxx7YDSW7b\nNP626a5vVyR5cloW9+kkP1dVF0w3Ovi5aQwAAGAhVk41oao+luRnklxYVeuZ3bXtcJJPVNV1SR5J\ncs00/c4kb0xyNMm3krw9Sbr7iar6vSRfmOb9bnefeBMFAACAs+aU4ae73/IcL115krmd5Prn+Jyb\nk9x8RtUBAABsk+2+4QEAAMCOJPwAAABDEH4AAIAhCD8AAMAQhB8AAGAIwg8AADAE4QcAABiC8AMA\nAAxB+AEAAIYg/AAAAEMQfgAAgCEIPwAAwBCEHwAAYAjCDwAAMAThBwAAGILwAwAADEH4AQAAhiD8\nAAAAQxB+AACAIQg/AADAEIQfAABgCMIPAAAwhJVlFwAAsGxrh+5Y2L4ePnz1wvYFPJMzPwAAwBCE\nHwAAYAjCDwAAMAThBwAAGILwAwAADGGu8FNVv1lV91fVV6vqY1X1gqq6uKruqaoHq+pvqurcae73\nTdtHp9fXtuMLAAAAOB1bDj9VtSfJryfZ190/nuScJNcm+aMk7+/uS5J8I8l101uuS/KN7n5lkvdP\n8wAAABZi3mVvK0m+v6pWkpyX5LEkr0ty6/T6LUneND3fP21nev3Kqqo59w8AAHBathx+uvvfkvxx\nkkcyCz1PJrk3yTe7+6lp2nqSPdPzPUmOTe99apr/kq3uHwAA4EzMs+ztgszO5lyc5IeSvDDJVSeZ\n2htveZ7XNn/uwao6UlVHjh8/vtXyAOCs0KcAdq95lr39bJJ/6e7j3f2/ST6V5KeSnD8tg0uSvUke\nnZ6vJ7koSabXfzDJEyd+aHff2N37unvf6urqHOUBwPbTpwB2r3nCzyNJrqiq86Zrd65M8rUkn03y\n5mnOgSS3Tc9vn7Yzvf6Z7n7WmR8AAICzYZ5rfu7J7MYFX0zylemzbkzy7iTvqqqjmV3Tc9P0lpuS\nvGQaf1eSQ3PUDQAAcEZWTj3luXX3+5K874Thh5JcfpK5/5Pkmnn2BwAAsFXz3uoaAABgVxB+AACA\nIQg/AADAEIQfAABgCMIPAAAwBOEHAAAYgvADAAAMQfgBAACGIPwAAABDEH4AAIAhCD8AAMAQhB8A\nAGAIwg8AADAE4QcAABiC8AMAAAxB+AEAAIYg/AAAAEMQfgAAgCEIPwAAwBCEHwAAYAjCDwAAMATh\nBwAAGILwAwAADEH4AQAAhiD8AAAAQxB+AACAIQg/AADAEIQfAABgCHOFn6o6v6purap/qqoHquo1\nVfXiqrqrqh6cHi+Y5lZVfaiqjlbVl6vqsu35EgAAAE5tZc73fzDJ33X3m6vq3CTnJXlvkru7+3BV\nHUpyKMm7k1yV5JLpz08muWF6BAazduiOhezn4cNXL2Q/AMDusOUzP1X1oiQ/neSmJOnub3f3N5Ps\nT3LLNO2WJG+anu9P8pGe+XyS86vq5VuuHAAA4AzMs+ztR5IcT/IXVXVfVX24ql6Y5GXd/ViSTI8v\nnebvSXJs0/vXpzEAAICzbp5lbytJLkvyju6+p6o+mNkSt+dSJxnrZ02qOpjkYJK84hWvmKM8eKZF\nLbVKdvZyK0vOdg7/LXan7e5Tvg8AFmeeMz/rSda7+55p+9bMwtDXN5azTY+Pb5p/0ab3703y6Ikf\n2t03dve+7t63uro6R3kAsP30KYDda8vhp7v/PcmxqnrVNHRlkq8luT3JgWnsQJLbpue3J3nbdNe3\nK5I8ubE8DgAA4Gyb925v70jy0elObw8leXtmgeoTVXVdkkeSXDPNvTPJG5McTfKtaS4AAMBCzBV+\nuvtLSfad5KUrTzK3k1w/z/4AAAC2aq5fcgoAALBbzLvsDQBgLu7GCSyKMz8AAMAQhB8AAGAIwg8A\nADAE4QcAABiC8AMAAAxB+AEAAIYg/AAAAEMQfgAAgCEIPwAAwBCEHwAAYAjCDwAAMISVZRcAAAA8\n09qhOxayn4cPX72Q/ewUzvwAAABDEH4AAIAhCD8AAMAQhB8AAGAIwg8AADAE4QcAABiCW10DQ3IL\nUQAYj/AzgEX9kJf4QQ8AgJ3LsjcAAGAIwg8AADAEy95ggSxBBGAn06f4bufMDwAAMAThBwAAGILw\nAwAADEH4AQAAhjB3+Kmqc6rqvqr622n74qq6p6oerKq/qapzp/Hvm7aPTq+vzbtvAACA07Udd3t7\nZ5IHkrxo2v6jJO/v7o9X1Z8nuS7JDdPjN7r7lVV17TTvF7dh/wAAsG3c9e6711zhp6r2Jrk6yR8k\neVdVVZLXJfmlacotSX4ns/Czf3qeJLcm+dOqqu7ueWoAAAC236JC4CID4LzL3j6Q5LeT/N+0/ZIk\n3+zup6bt9SR7pud7khxLkun1J6f5z1BVB6vqSFUdOX78+JzlAcD20qcAdq8th5+q+vkkj3f3vZuH\nTzK1T+O1pwe6b+zufd29b3V1davlAcBZoU8B7F7zLHt7bZJfqKo3JnlBZtf8fCDJ+VW1Mp3d2Zvk\n0Wn+epKLkqxX1UqSH0zyxBz7BwAAOG1bPvPT3e/p7r3dvZbk2iSf6e5fTvLZJG+eph1Ictv0/PZp\nO9Prn3G9DwAAsCjbcbe3E707ycer6veT3Jfkpmn8piR/VVVHMzvjc+1Z2PczuFMHALBb+LkFzr5t\nCT/d/bkkn5ueP5Tk8pPM+Z8k12zH/gAAAM7U3L/kFAAAYDcQfgAAgCGcjWt+AABgS1z7xNnkzA8A\nADAE4QcAABiC8AMAAAxB+AEAAIYg/AAAAEMQfgAAgCEIPwAAwBCEHwAAYAjCDwAAMAThBwAAGILw\nAwAADEH4AQAAhiD8AAAAQxB+AACAIQg/AADAEIQfAABgCMIPAAAwBOEHAAAYgvADAAAMQfgBAACG\nIPwAAABDEH4AAIAhCD8AAMAQhB8AAGAIwg8AADCELYefqrqoqj5bVQ9U1f1V9c5p/MVVdVdVPTg9\nXjCNV1V9qKqOVtWXq+qy7foiAAAATmWeMz9PJfmt7v6xJFckub6qLk1yKMnd3X1Jkrun7SS5Kskl\n05+DSW6YY98AAABnZMvhp7sf6+4vTs//K8kDSfYk2Z/klmnaLUneND3fn+QjPfP5JOdX1cu3XDkA\nAMAZ2JZrfqpqLcmrk9yT5GXd/VgyC0hJXjpN25Pk2Ka3rU9jAAAAZ93c4aeqfiDJJ5P8Rnf/5/NN\nPclYn+TzDlbVkao6cvz48XnLA4BtpU8B7F5zhZ+q+t7Mgs9Hu/tT0/DXN5azTY+PT+PrSS7a9Pa9\nSR498TO7+8bu3tfd+1ZXV+cpDwC2nT4FsHvNc7e3SnJTkge6+082vXR7kgPT8wNJbts0/rbprm9X\nJHlyY3kcAADA2bYyx3tfm+StSb5SVV+axt6b5HCST1TVdUkeSXLN9NqdSd6Y5GiSbyV5+xz7BgAA\nOCNbDj/d/Y85+XU8SXLlSeZ3kuu3uj8AAIB5bMvd3gAAAHY64QcAABjCPNf8wGlbO3THwvb18OGr\nF7YvAAB2D2d+AACAIQg/AADAEIQfAABgCMIPAAAwBOEHAAAYgvADAAAMQfgBAACGIPwAAABDEH4A\nAIAhCD8AAMAQhB8AAGAIwg8AADAE4QcAABiC8AMAAAxB+AEAAIYg/AAAAEMQfgAAgCEIPwAAwBCE\nHwAAYAjCDwAAMAThBwAAGILwAwAADEH4AQAAhiD8AAAAQxB+AACAIawseodV9YYkH0xyTpIPd/fh\nRdcAADxt7dAdC9nPw4evXsh+AJ7LQs/8VNU5Sf4syVVJLk3ylqq6dJE1AAAAY1r0srfLkxzt7oe6\n+9tJPp5k/4JrAAAABrTo8LMnybFN2+vTGAAAwFlV3b24nVVdk+T13f2r0/Zbk1ze3e/YNOdgkoPT\n5quS/PPCCnzahUn+Ywn73WkcB8dgg+MwM+px+OHuXl12ETuFPrWjOA4zjoNjsGHk43BavWrR4ec1\nSX6nu18/bb8nSbr7DxdWxGmoqiPdvW/ZdSyb4+AYbHAcZhwHdgrfizOOw4zj4BhscBxObdHL3r6Q\n5JKquriqzk1ybZLbF1wDAAAwoIXe6rq7n6qqX0vy6cxudX1zd9+/yBoAAIAxLfz3/HT3nUnuXPR+\nz9CNyy5gh3AcHIMNjsOM48BO4XtxxnGYcRwcgw2Owyks9JofAACAZVn0NT8AAABLIfxsUlVvqKp/\nrqqjVXVo2fUsQ1VdVFWfraoHqur+qnrnsmtapqo6p6ruq6q/XXYty1JV51fVrVX1T9P3xWuWXdMy\nVNVvTn8nvlpVH6uqFyy7JsajT+lTJ9Kn9KkN+tTpEX4mVXVOkj9LclWSS5O8paouXW5VS/FUkt/q\n7h9LckWS6wc9DhvemeSBZRexZB9M8nfd/aNJfiIDHo+q2pPk15Ps6+4fz+yGLdcutypGo099hz71\nTPqUPqVPnQHh52mXJzna3Q9197eTfDzJ/iXXtHDd/Vh3f3F6/l+Z/Q9kz3KrWo6q2pvk6iQfXnYt\ny1JVL0ry00luSpLu/nZ3f3O5VS3NSpLvr6qVJOcleXTJ9TAefSr61Gb6lD51An3qNAg/T9uT5Nim\n7fUM+j/TDVW1luTVSe5ZbiVL84Ekv53k/5ZdyBL9SJLjSf5iWlbx4ap64bKLWrTu/rckf5zkkSSP\nJXmyu/9+uVUxIH3qBPqUPhV9Kok+dSaEn6fVScaGvRVeVf1Akk8m+Y3u/s9l17NoVfXzSR7v7nuX\nXcuSrSS5LMkN3f3qJP+dZLjrDKrqgsz+hf3iJD+U5IVV9SvLrYoB6VOb6FP61ESfij51JoSfp60n\nuWjT9t4Merqwqr43s4by0e7+1LLrWZLXJvmFqno4s6Ulr6uqv15uSUuxnmS9uzf+VfXWzJrMaH42\nyb909/Hu/t8kn0ryU0uuifHoUxN9Kok+tUGfmtGnTpPw87QvJLmkqi6uqnMzu0js9iXXtHBVVZmt\nm32gu/9k2fUsS3e/p7v3dvdaZt8Ln+nu4f4Fpbv/PcmxqnrVNHRlkq8tsaRleSTJFVV13vR35MoM\neEEtS6dPRZ/aoE/N6FPfoU+dppVlF7BTdPdTVfVrST6d2R0ybu7u+5dc1jK8Nslbk3ylqr40jb23\nu+9cYk0s1zuSfHT6YeuhJG9fcj0L1933VNWtSb6Y2Z2m7ovfos2C6VPfoU9xIn1Knzpt1T3scmEA\nAGAglr0BAABDEH4AAIAhCD8AAMAQhB8AAGAIwg8AADAE4QcAABiC8AMAAAxB+AEAAIbw/xmOzMMM\nS2pzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11850aa58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_test_tree_spread(bz, model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Linear(in_features=784, out_features=1)\n",
       "  (1): Linear(in_features=784, out_features=1)\n",
       "  (2): Linear(in_features=784, out_features=1)\n",
       "  (3): Linear(in_features=784, out_features=1)\n",
       "  (4): Linear(in_features=784, out_features=1)\n",
       "  (5): Linear(in_features=784, out_features=1)\n",
       "  (6): Linear(in_features=784, out_features=1)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Linear(in_features=784, out_features=1)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
