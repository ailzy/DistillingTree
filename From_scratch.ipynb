{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distilling a Neural Network into a soft decision tree\n",
    "\n",
    "https://arxiv.org/pdf/1711.09784.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup our dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "class myNestedImgDataset(Dataset):\n",
    "    def __init__(self, dir_path, transform=None, test=False):\n",
    "        self.dir_path = dir_path\n",
    "        self.transform = transform\n",
    "        self.classes = [x for x in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path,x))]\n",
    "        self.img_paths = []\n",
    "        self.labels = []\n",
    "        self.test = test\n",
    "        if self.test:\n",
    "            class_img_paths = [os.path.join(dir_path,x) for x in os.listdir(dir_path)]\n",
    "            self.img_paths.extend(class_img_paths)\n",
    "        else:\n",
    "            for class_idx, folder_name in enumerate(self.classes):\n",
    "                prefix = os.path.join(dir_path,folder_name)\n",
    "                class_img_paths = [os.path.join(prefix,x) for x in os.listdir(prefix)]\n",
    "                self.img_paths.extend(class_img_paths)\n",
    "                self.labels.extend(np.ones(len(class_img_paths))*class_idx)\n",
    "            \n",
    "            self.labels = [int(x) for x in self.labels]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if self.test:\n",
    "            return torch.FloatTensor(plt.imread(self.img_paths[idx])), None\n",
    "        else:\n",
    "            return torch.FloatTensor(plt.imread(self.img_paths[idx])), self.labels[idx]\n",
    "    \n",
    "    def show(self,idx):\n",
    "        return plt.imshow(mpimg.imread(self.img_paths[idx]), cmap='Greys')\n",
    "    \n",
    "\n",
    "class Img2FlatVec(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset= dataset\n",
    "        self.n = len(self.dataset)\n",
    "        samp_img, _ = self.dataset[1]\n",
    "        self.h, self.w = samp_img.shape\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        x, y = self.dataset[idx]\n",
    "        x_flat = x \n",
    "        return x.view(self.h*self.w)/255, y\n",
    "\n",
    "    def __len__(self): return self.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use soft binary decision trees trained with mini-batch gradient descent, where\n",
    "each inner node i has a learned filter **`w_i`** and a bias **`b_i`**\n",
    ", and each leaf node has a learned distribution **`Q`**. At each inner node, the probability of taking the\n",
    "rightmost branch is:\n",
    "\n",
    "$$p_i(x) = \\sigma (xw_i + b_i)$$\n",
    "\n",
    "where **`x`** is the input to the model and **$\\sigma$** is the sigmoid logistic function.\n",
    "\n",
    "For example if we have a 2 x 2 image, and 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = torch.randn((2*2,3))\n",
    "x = torch.randn((2*2,1))\n",
    "b = torch.randn((2*2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " -0.4307 -0.3382  0.8263\n",
       " -1.1472 -0.0171  1.4875\n",
       " -0.6736  0.3648  0.2261\n",
       "  1.3679 -0.9086 -1.7713\n",
       " [torch.FloatTensor of size 4x3], \n",
       " -2.1332\n",
       " -1.3903\n",
       "  0.7608\n",
       " -0.5484\n",
       " [torch.FloatTensor of size 4x1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.5247 -0.7219 -3.2061\n",
       " 3.0537  1.4827 -0.6091\n",
       "-0.2432  0.5469  0.4414\n",
       "-1.2541 -0.0056  0.4675\n",
       "[torch.FloatTensor of size 4x3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x*w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mul() received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\n * (float other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (Variable other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-226-a49ccdc9309b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: mul() received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\n * (float other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n * (Variable other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "F.sigmoid(x*w + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here **`x`** is the input to the model and Ïƒ is the sigmoid logistic function.\n",
    "This model is a hierarchical mixture of experts [Jordan and Jacobs, 1994], but\n",
    "each expert is a actually a bigot who does not look at the data after training, and\n",
    "therefore always produces the same distribution. The model learns a hierarchy of\n",
    "filters that are used to assign each example to a particular bigot with a particular\n",
    "path probability, and each bigot learns a simple, static distribution over the\n",
    "possible output classes, **`k`**.\n",
    "\n",
    "$$ Q_k^l = \\frac{exp(\\phi_k^l)}{\\sum_{k'}{exp(\\phi_k^l)}}$$\n",
    "\n",
    "where `Q` denotes the probability distribution at the lth leaf, and each $\\phi$. is a learned parameter at that leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_Q = torch.zeros(2*2,1)\n",
    "max_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.4271\n",
       " 0.7438\n",
       " 0.0892\n",
       "[torch.FloatTensor of size 3x1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_init = torch.rand(3,1)\n",
    "Q_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      " 0.4271  0.7438  0.0892\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q_nn = nn.Parameter(Q_init.view(1,-1))\n",
    "print(Q_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.3240  0.4448  0.2311\n",
       "[torch.FloatTensor of size 1x3]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = torch.nn.Softmax(dim=1)\n",
    "sm(Q_nn.view(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototyping a Soft Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bz = 64\n",
    "input_dim = 28*28\n",
    "no_classes = 10\n",
    "max_depth = 8\n",
    "epochs = 4\n",
    "lr = 0.01\n",
    "lmbda = 0.1\n",
    "momentum = 0.5\n",
    "seed = 1\n",
    "cuda = False\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototyping a `LeafNode`\n",
    "\n",
    "The leaf node will give the softmax calculation for **$n$** number of classes. For tree consistency, it will also implement a `reset` method, and a `forward` method. \n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Lisc_lipy.jpg/220px-Lisc_lipy.jpg' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LeafNode():\n",
    "    def __init__(self,batch_size, no_classes, cuda=False):\n",
    "        self.param = torch.randn(no_classes)\n",
    "        if cuda:\n",
    "            self.param = self.param.cuda()\n",
    "        self.param = nn.Parameter(self.param)\n",
    "        self.batch_size = batch_size\n",
    "        self.leaf = True\n",
    "        self.softmax_func = nn.Softmax(1)\n",
    "        \n",
    "    def forward(self):\n",
    "        return(self.softmax_func(self.param.view(1,-1)))\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "    def calc_prob(self, x, path_prob):\n",
    "        # the x is kept there for consistency\n",
    "        # is because its at the tail of the tree\n",
    "        Q = self.forward()\n",
    "        \n",
    "        ## broad casting for every image in the batchsize, same probability for number of classes\n",
    "        Q = Q.expand(self.batch_size, no_classes)\n",
    "        return([[path_prob,Q]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our `LeafNode Object`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ============ Testing  leaf node softmax ============\n",
      "Variable containing:\n",
      " 0.4693  0.2613  0.2694\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n",
      "============ Testing probability calculation per item in batch ============\n",
      "[[Variable containing:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 6x1]\n",
      ", Variable containing:\n",
      " 0.4693  0.2613  0.2694\n",
      " 0.4693  0.2613  0.2694\n",
      " 0.4693  0.2613  0.2694\n",
      " 0.4693  0.2613  0.2694\n",
      " 0.4693  0.2613  0.2694\n",
      " 0.4693  0.2613  0.2694\n",
      "[torch.FloatTensor of size 6x3]\n",
      "]]\n"
     ]
    }
   ],
   "source": [
    "# given the following parameters ...\n",
    "batch_size = 6 # say 15 images per batch\n",
    "no_classes = 3\n",
    "input_dim = 2*2\n",
    "path_prob = Variable(torch.ones(batch_size, 1))\n",
    "x = torch.randn(batch_size,input_dim)    # approximating some images\n",
    "\n",
    "# create a leaf node\n",
    "ln = LeafNode(batch_size, no_classes, False)\n",
    "print(' ============ Testing  leaf node softmax ============')\n",
    "print(ln.forward())\n",
    "\n",
    "\n",
    "print('============ Testing probability calculation per item in batch ============')\n",
    "print(ln.calc_prob(x, path_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on regularization\n",
    "\n",
    "To avoid getting stuck at poor solutions during the training, we introduced a penalty term that encourage each internal node to make equal use of both left and right sub-tress. Without this penalty, the tree tended to get stuck on plateaus in which one or more of the internal nodes always assigned almost all the probability to one of its sub-tres and the gradient of the logistic for this decision was always very close to zero. \n",
    "\n",
    "The penalty is the cross entropy between teh diered average distribution (0.5, 0.5) for the two sub-tress and teh actual average distribution $\\alpha$. Where $\\alpha$ for a node $i$ is given by:\n",
    "\n",
    "$$\\alpha_i = \\frac {\\sum_x P^i(x)p_i(x)}{\\sum_x P^i(x)}$$\n",
    "\n",
    "where $P^i(x)$ is the path probability from teh root node to node i. The penalty summed over all internal nodes is then:\n",
    "\n",
    "$$ C = -\\lambda \\sum 0.5 log(\\alpha_i) + 0.5 log(1-\\alpha_i)$$\n",
    "\n",
    "#### On Lambda:\n",
    "\n",
    "is a hyper-parameter that determines the strength of the penalty and is set prior to training. We found that we achieved better test accuracy results when the strength of the penalty decayed exponentially with the depth d of the\n",
    "node in the tree so that it was proportional to $2^{âˆ’d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototyping Inner node\n",
    "\n",
    "<img src='http://mimi.kaktusteam.de/uploads/pics/simple_tree.jpg' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InnerNode():\n",
    "    def __init__(self, depth, batch_size, input_dim, no_classes, lmbda,cuda=False,tree_depth=1):\n",
    "        self.depth = depth\n",
    "        self.tree_depth = tree_depth\n",
    "        self.input_dim = input_dim\n",
    "        self.no_classes = no_classes\n",
    "        self.cuda = cuda\n",
    "        self.batch_size = batch_size\n",
    "        self.fc_layer = nn.Linear(self.input_dim,1)\n",
    "        \n",
    "        self.beta = torch.randn(1)\n",
    "        if cuda:\n",
    "            self.beta = self.beta.cuda()\n",
    "        self.beta = nn.Parameter(self.beta)\n",
    "        self.leaf = False\n",
    "        self.prob = None\n",
    "        \n",
    "        # for regularization the strength of the penalty decays (as mentioned in the paper)\n",
    "        self.lmbda = lmbda * 2** (-tree_depth)\n",
    "        \n",
    "        if depth > 1:\n",
    "            # recursive part\n",
    "            self.left = InnerNode(depth-1, batch_size, input_dim, no_classes, lmbda,cuda, tree_depth+1)\n",
    "            self.right = InnerNode(depth-1, batch_size, input_dim, no_classes, lmbda,cuda, tree_depth+1)\n",
    "        else:\n",
    "            # when the depth is exactly 1, then only have 2 leaf nodes\n",
    "            self.left = LeafNode(batch_size, no_classes, cuda)\n",
    "            self.right = LeafNode(batch_size, no_classes, cuda)\n",
    "            \n",
    "        self.all_leaf_probs = []\n",
    "        self.prob_dict = {}\n",
    "        self.penalties = []\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.all_leaf_probs = []\n",
    "        self.penalties = []\n",
    "        \n",
    "        # recursively \n",
    "        self.left.reset()\n",
    "        self.right.reset()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):  \n",
    "        # this is the branch probability calculation\n",
    "        return (F.sigmoid(self.beta*self.fc_layer(x)))\n",
    "    \n",
    "    def calc_prob(self, x, path_prob):\n",
    "        # calculate the inner probability with sigmoid\n",
    "        self.prob = self.forward(x)\n",
    "        \n",
    "        # store the current path probability\n",
    "        self.path_prob = path_prob\n",
    "        \n",
    "        # pull the Q prob distributions from left and right leaves\n",
    "        left_probs = self.left.calc_prob(x, path_prob*(1-self.prob))\n",
    "        right_probs = self.right.calc_prob(x, path_prob*(self.prob))\n",
    "        \n",
    "        # append them to master list\n",
    "        self.all_leaf_probs.extend(left_probs)\n",
    "        self.all_leaf_probs.extend(right_probs)\n",
    "        \n",
    "        # return only the leaf prob distributions\n",
    "        return (self.all_leaf_probs)\n",
    "      \n",
    "    def select_next(self, x):\n",
    "        # the probability is defined as probability of the right side\n",
    "        prob = self.forward(x)\n",
    "        \n",
    "        if prob < 0.5: \n",
    "            return(self.left, prob)\n",
    "        else:\n",
    "            return(self.right, prob)\n",
    "    \n",
    "    \n",
    "    def get_penalty(self):\n",
    "        alpha_num = torch.sum(self.path_prob*self.prob) \n",
    "        alpha_den = torch.sum(self.path_prob)\n",
    "        alpha = alpha_num / alpha_den\n",
    "        C_i = -self.lmbda * 0.5 * (torch.log(alpha) + torch.log(1-alpha))\n",
    "        \n",
    "        self.penalties.append(C_i)\n",
    "        if not self.left.leaf:\n",
    "            left_C_i = self.left.get_penalty()\n",
    "            right_C_i = self.right.get_penalty()\n",
    "            self.penalties.extend(left_C_i)\n",
    "            self.penalties.extend(right_C_i)\n",
    "        return (self.penalties)\n",
    "    \n",
    "    def collect_params(self):\n",
    "        self.module_list = []\n",
    "        self.param_list = []\n",
    "        self.module_list.append(self.fc_layer)\n",
    "        self.param_list.append(self.beta)        \n",
    "        if self.left.leaf:\n",
    "            self.param_list.append(self.left.param)\n",
    "        else:\n",
    "            mod, params = self.left.collect_params()\n",
    "            self.module_list.extend(mod)\n",
    "            self.param_list.extend(params)\n",
    "            \n",
    "        if self.right.leaf:\n",
    "            self.param_list.append(self.right.param)\n",
    "        else:\n",
    "            mod, params = self.right.collect_params()\n",
    "            self.module_list.extend(mod)\n",
    "            self.param_list.extend(params)\n",
    "        \n",
    "        return(self.module_list, self.param_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out our inner node class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== Probabilities and Path Probabilities =============== \n",
      "Number of leafs: 2\n",
      "=== path Prob ===\n",
      "Variable containing:\n",
      " 0.4057\n",
      " 0.3627\n",
      " 0.5319\n",
      " 0.5258\n",
      " 0.4195\n",
      " 0.3670\n",
      " 0.5704\n",
      " 0.3298\n",
      "[torch.FloatTensor of size 8x1]\n",
      "\n",
      "=== Q Dist ===\n",
      "Variable containing:\n",
      " 0.1501  0.0525  0.0990  0.1238  0.0192  0.0679  0.0708  0.1899  0.1823  0.0445\n",
      " 0.1501  0.0525  0.0990  0.1238  0.0192  0.0679  0.0708  0.1899  0.1823  0.0445\n",
      " 0.1501  0.0525  0.0990  0.1238  0.0192  0.0679  0.0708  0.1899  0.1823  0.0445\n",
      " 0.1501  0.0525  0.0990  0.1238  0.0192  0.0679  0.0708  0.1899  0.1823  0.0445\n",
      " 0.1501  0.0525  0.0990  0.1238  0.0192  0.0679  0.0708  0.1899  0.1823  0.0445\n",
      " 0.1501  0.0525  0.0990  0.1238  0.0192  0.0679  0.0708  0.1899  0.1823  0.0445\n",
      " 0.1501  0.0525  0.0990  0.1238  0.0192  0.0679  0.0708  0.1899  0.1823  0.0445\n",
      " 0.1501  0.0525  0.0990  0.1238  0.0192  0.0679  0.0708  0.1899  0.1823  0.0445\n",
      "[torch.FloatTensor of size 8x10]\n",
      "\n",
      "=== path Prob ===\n",
      "Variable containing:\n",
      " 0.5943\n",
      " 0.6373\n",
      " 0.4681\n",
      " 0.4742\n",
      " 0.5805\n",
      " 0.6330\n",
      " 0.4296\n",
      " 0.6702\n",
      "[torch.FloatTensor of size 8x1]\n",
      "\n",
      "=== Q Dist ===\n",
      "Variable containing:\n",
      " 0.1125  0.0373  0.0380  0.1939  0.0167  0.0717  0.1972  0.1697  0.0843  0.0788\n",
      " 0.1125  0.0373  0.0380  0.1939  0.0167  0.0717  0.1972  0.1697  0.0843  0.0788\n",
      " 0.1125  0.0373  0.0380  0.1939  0.0167  0.0717  0.1972  0.1697  0.0843  0.0788\n",
      " 0.1125  0.0373  0.0380  0.1939  0.0167  0.0717  0.1972  0.1697  0.0843  0.0788\n",
      " 0.1125  0.0373  0.0380  0.1939  0.0167  0.0717  0.1972  0.1697  0.0843  0.0788\n",
      " 0.1125  0.0373  0.0380  0.1939  0.0167  0.0717  0.1972  0.1697  0.0843  0.0788\n",
      " 0.1125  0.0373  0.0380  0.1939  0.0167  0.0717  0.1972  0.1697  0.0843  0.0788\n",
      " 0.1125  0.0373  0.0380  0.1939  0.0167  0.0717  0.1972  0.1697  0.0843  0.0788\n",
      "[torch.FloatTensor of size 8x10]\n",
      "\n",
      "== checking total probability for each of the img in the batch (rows)\n",
      "Variable containing:\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "[torch.FloatTensor of size 8x1]\n",
      "\n",
      "=============== Penalities =============== \n",
      "[Variable containing:\n",
      "1.00000e-03 *\n",
      "  3.5031\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# given the following parameters ...\n",
    "batch_size = 8 # say 15 images per batch\n",
    "no_classes = 10\n",
    "input_dim = 2*2\n",
    "lmbda = 0.01\n",
    "depth = 1\n",
    "path_prob = Variable(torch.ones(batch_size, 1))\n",
    "x = Variable(torch.randn(batch_size,input_dim)) # approximating some images\n",
    "\n",
    "inner_N = InnerNode(depth, batch_size, input_dim, no_classes,lmbda)\n",
    "print('=============== Probabilities and Path Probabilities =============== ')\n",
    "res = inner_N.calc_prob(x, path_prob)\n",
    "print('Number of leafs: %d' % len(res))\n",
    "#print(res)\n",
    "total = Variable(torch.zeros((batch_size,1)))\n",
    "for row in res:\n",
    "    print('=== path Prob ===')\n",
    "    print(row[0])\n",
    "    print('=== Q Dist ===')\n",
    "    print(row[1])\n",
    "    total += row[0]\n",
    "print('== checking total probability for each of the img in the batch (rows)')\n",
    "print(total)\n",
    "\n",
    "print('=============== Penalities =============== ')\n",
    "print(inner_N.get_penalty())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bz = 3\n",
    "mnist_trn = myNestedImgDataset('/Users/timlee/data/MNIST/trn/')\n",
    "trn_vec = Img2FlatVec(mnist_trn)\n",
    "trn_dl = DataLoader(trn_vec, batch_size=bz, shuffle=True, num_workers=4)\n",
    "x_test, y_test = iter(trn_dl).next()\n",
    "x_var = Variable(x_test)\n",
    "y_var = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2\n",
       " 4\n",
       " 3\n",
       "[torch.LongTensor of size 3]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      " 4\n",
      " 3\n",
      "[torch.LongTensor of size 3]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.FloatTensor of size 10]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def target2onehot(batch_size, no_classes, y):  \n",
    "    \"\"\"\n",
    "    takes in a single vector \n",
    "    \"\"\"\n",
    "    template = torch.FloatTensor(batch_size, no_classes)\n",
    "    template.zero_()\n",
    "    template = Variable(template)\n",
    "    if type(y) == torch.autograd.variable.Variable:\n",
    "        target = y\n",
    "    else:\n",
    "        target = Variable(y)\n",
    "    template.scatter_(1,target.view(-1,1),1)\n",
    "    \n",
    "    # was getting nan's in some of the small numbers\n",
    "    template[template != template] = 0\n",
    "    return template\n",
    "\n",
    "bz = 3\n",
    "no_classes = 10\n",
    "t2o_test = target2onehot(bz, no_classes, y_test)\n",
    "print(y_test)\n",
    "torch.sum(t2o_test,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.2440  0.0179  0.0367  0.0441  0.0869  0.2843  0.0697  0.0750  0.0745  0.0669\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n",
      "Variable containing:\n",
      " 0.2440  0.0179  0.0367  0.0441  0.0869  0.2843  0.0697  0.0750  0.0745  0.0669\n",
      " 0.2440  0.0179  0.0367  0.0441  0.0869  0.2843  0.0697  0.0750  0.0745  0.0669\n",
      " 0.2440  0.0179  0.0367  0.0441  0.0869  0.2843  0.0697  0.0750  0.0745  0.0669\n",
      "[torch.FloatTensor of size 3x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr = Variable(torch.randn(no_classes))\n",
    "sm = nn.Softmax(1)\n",
    "Q_sample = sm(tr.view(1,-1))\n",
    "print(Q_sample)\n",
    "Q_batch_sample = Q_sample.expand(bz, no_classes)\n",
    "print(Q_batch_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    0     0     1     0     0     0     0     0     0     0\n",
       "    0     0     0     0     1     0     0     0     0     0\n",
       "    0     0     0     1     0     0     0     0     0     0\n",
       "[torch.FloatTensor of size 3x10]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2o_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10]) torch.Size([3, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: wrong matrix size, batch1: 1x10, batch2: 1x10 at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1638",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-a464d8adf5a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#print(aa)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: wrong matrix size, batch1: 1x10, batch2: 1x10 at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1638"
     ]
    }
   ],
   "source": [
    "print(Q_batch_sample.shape, t2o_test.shape)\n",
    "\n",
    "bb = t2o_test.view(bz,1,no_classes)\n",
    "#print(bb)\n",
    "\n",
    "aa = Q_batch_sample.contiguous().view(bz, no_classes, 1)\n",
    "#print(aa)\n",
    "\n",
    "torch.bmm(bb, torch.log(bb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "1.00000e-02 *\n",
       "   3.6699\n",
       "   3.6699\n",
       "   3.6699\n",
       "   3.6699\n",
       "   3.6699\n",
       "   3.6699\n",
       "   3.6699\n",
       "   3.6699\n",
       "\n",
       "(1 ,.,.) = \n",
       "1.00000e-02 *\n",
       "   8.6950\n",
       "   8.6950\n",
       "   8.6950\n",
       "   8.6950\n",
       "   8.6950\n",
       "   8.6950\n",
       "   8.6950\n",
       "   8.6950\n",
       "\n",
       "(2 ,.,.) = \n",
       "1.00000e-02 *\n",
       "   4.4149\n",
       "   4.4149\n",
       "   4.4149\n",
       "   4.4149\n",
       "   4.4149\n",
       "   4.4149\n",
       "   4.4149\n",
       "   4.4149\n",
       "[torch.FloatTensor of size 3x8x1]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_prob = Variable(torch.ones(batch_size, 1))\n",
    "resr = path_prob * torch.bmm(bb, aa)\n",
    "resr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-02 *\n",
       "  5.5933\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "    0\n",
      "    0\n",
      "    1\n",
      "    1\n",
      "    1\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -1.4108\n",
      " -4.0234\n",
      " -3.3050\n",
      " -3.1202\n",
      " -2.4424\n",
      " -1.2576\n",
      " -2.6641\n",
      " -2.5905\n",
      " -2.5970\n",
      " -2.7048\n",
      "\n",
      "(1 ,.,.) = \n",
      " -1.4108\n",
      " -4.0234\n",
      " -3.3050\n",
      " -3.1202\n",
      " -2.4424\n",
      " -1.2576\n",
      " -2.6641\n",
      " -2.5905\n",
      " -2.5970\n",
      " -2.7048\n",
      "\n",
      "(2 ,.,.) = \n",
      " -1.4108\n",
      " -4.0234\n",
      " -3.3050\n",
      " -3.1202\n",
      " -2.4424\n",
      " -1.2576\n",
      " -2.6641\n",
      " -2.5905\n",
      " -2.5970\n",
      " -2.7048\n",
      "[torch.FloatTensor of size 3x10x1]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 1: expected 3D tensor, got 2D at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1630",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-7e4314b38d5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mpath_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mnode_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_batch_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-198-7e4314b38d5f>\u001b[0m in \u001b[0;36mnode_loss\u001b[0;34m(y, Q, path_prob, batch_size, no_classes)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlogQ_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogQ_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mTQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogQ_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTQ\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpath_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 1: expected 3D tensor, got 2D at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1630"
     ]
    }
   ],
   "source": [
    "def node_loss( y, Q, path_prob, batch_size, no_classes):\n",
    "    \"\"\"\n",
    "    takes in target label y (batch size x 1)\n",
    "    and probability dist (batch size x class)\n",
    "\n",
    "    returns a (batch size x 1) which is the loss constant per batch size\n",
    "    \"\"\"\n",
    "    target = target2onehot(batch_size, no_classes, y)\n",
    "    \n",
    "    T_k = torch.sum(target,dim=0).view(-1,1)\n",
    "    print(T_k)\n",
    "    logQ_k = torch.log(Q).view(batch_size, no_classes, 1)\n",
    "    print(logQ_k)\n",
    "    TQ = torch.bmm(T_k,logQ_k).view(-1,1)\n",
    "    print(TQ)\n",
    "    return(TQ*path_prob)\n",
    "\n",
    "\n",
    "path_prob = Variable(torch.ones(bz, 1))\n",
    "node_loss(y_test, Q_batch_sample, path_prob, bz, no_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototyping the decision tree\n",
    "\n",
    "The loss function:\n",
    "\n",
    "$$L(x) = -log(\\sum_{leaf nodes} P^l(x)\\sum_k T_k log{Q_k^l})$$\n",
    "\n",
    "- $Q_k^l$ - represents the distribution of probabilities for `k` classes at `l` leaf\n",
    "- $T_k$ - represents the target class (think one hot encoding)\n",
    "- $P^l(x)$ - represents the path probability at that leaf (this will be treated as compound prob)\n",
    "\n",
    "\n",
    "1. **`Q_k`** - the distribution matrix size is `batch_size x classes`\n",
    "2. **`T_k`** - the target matrix size is `batch size x classes`\n",
    "3. **`path_prob`** - the path probability is `batch size x 1` per node\n",
    "\n",
    "#### Notes on Path Probability:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftDTree(nn.Module):\n",
    "    def __init__(self, \n",
    "                 batch_size = 64, \n",
    "                 input_dim = 28*28, \n",
    "                 no_classes = 10, \n",
    "                 max_depth=8,\n",
    "                 epochs = 4,\n",
    "                 lr = 0.01,\n",
    "                 lmbda = 0.1,\n",
    "                 momentum = 0.5,\n",
    "                 seed = 1,\n",
    "                 cuda = False,\n",
    "                 log_interval = 10\n",
    "                ):\n",
    "        \n",
    "        # parameters\n",
    "        super(SoftDTree, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = input_dim\n",
    "        self.no_classes = no_classes\n",
    "        self.max_depth = max_depth\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.lmbda = lmbda\n",
    "        self.momentum = momentum\n",
    "        self.seed = seed\n",
    "        self.cuda = cuda\n",
    "        self.log_interval = log_interval\n",
    "\n",
    "        #setup target structures\n",
    "        self.root = InnerNode(self.max_depth, self.batch_size, self.input_dim, self.no_classes, self.lmbda)\n",
    "       \n",
    "        \n",
    "        # collects all the parameters to optimize from the nested nodes\n",
    "        self.collect_params() \n",
    "\n",
    "        # training objects\n",
    "        self.optimizer = optim.SGD(self.parameters(), \n",
    "                                   lr=lr, \n",
    "                                   momentum=momentum)\n",
    "        \n",
    "        self.initialize()\n",
    "        self.test_acc = []\n",
    "        self.best_accuracy = 0.0\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        if cuda:\n",
    "            torch.cuda.manual_seed(seed)\n",
    "\n",
    "        \n",
    "        \n",
    "    def initialize(self, batch_size = None):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        self.target_onehot = torch.FloatTensor(batch_size, self.no_classes)\n",
    "        self.target_onehot = Variable(self.target_onehot)\n",
    "        self.path_prob_init = Variable(torch.ones(batch_size,1))\n",
    "        if cuda:\n",
    "            self.target_onehot = self.target_onehot.cuda()\n",
    "            self.path_prob_init = self.path_prob_init.cuda()\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    def target2onehot(self, y):  \n",
    "        \"\"\"\n",
    "        takes in a single vector \n",
    "        \"\"\"\n",
    "        template = torch.FloatTensor(self.batch_size, self.no_classes)\n",
    "        template.zero_()        \n",
    "        template = Variable(template)\n",
    "        if type(y) == torch.autograd.variable.Variable:\n",
    "            target = y\n",
    "        else:\n",
    "            target = Variable(y)\n",
    "        template.scatter_(1,target.view(-1,1),1)\n",
    "        template[template != template] = 0\n",
    "        return template\n",
    "    \n",
    "    \n",
    "    def node_loss(self, y, Q, path_prob):\n",
    "        \"\"\"\n",
    "        takes in target label y (batch size x 1)\n",
    "        and probability dist (batch size x class)\n",
    "        \n",
    "        returns a (batch size x 1) which is the loss constant per batch size\n",
    "        \"\"\"\n",
    "        target = self.target2onehot(y)\n",
    "        T_k = target.view(self.batch_size, 1, self.no_classes)\n",
    "        logQ_k = torch.log(Q).view(self.batch_size, self.no_classes, 1)\n",
    "        TQ = torch.bmm(T_k,logQ_k).view(-1,1)\n",
    "        return(TQ*path_prob)\n",
    "    \n",
    "    \n",
    "    def most_prob_Q(self, list_prob_n_Q):\n",
    "        \"\"\"\n",
    "        takes in a list of (path_prob, Q)\n",
    "        \n",
    "        based on the path_prob\n",
    "        \"\"\"\n",
    "        # will store the max node probability per batch\n",
    "        max_prob = [-1. for _ in range(self.batch_size)]\n",
    "        \n",
    "        # will store the most likely distribution\n",
    "        max_Q = [torch.zeros(self.no_classes) for _ in range(self.batch_size)]        \n",
    "        for (path_prob, Q) in list_prob_n_Q:\n",
    "            path_prob_numpy = path_prob.cpu().data.numpy().reshape(-1)\n",
    "            for i in range(self.batch_size):\n",
    "                if max_prob[i] < path_prob_numpy[i]:\n",
    "                    max_prob[i] = path_prob_numpy[i]\n",
    "                    max_Q[i] = Q[i]\n",
    "\n",
    "        return(max_prob, max_Q)\n",
    "    \n",
    "    \n",
    "    def calc_loss(self, x, y):\n",
    "\n",
    "        all_leaf_probs = self.root.calc_prob(x, self.path_prob_init)\n",
    "        \n",
    "        # based on the path (max prob), get the distribution \n",
    "        max_path_prob, max_Q = self.most_prob_Q(all_leaf_probs)        \n",
    "\n",
    "        total_loss = torch.mean(torch.sum(torch.stack([self.node_loss(y, Q, path_prob) for path_prob, Q in all_leaf_probs]), dim=0))\n",
    "        total_C = torch.sum(torch.stack(self.root.get_penalty()))\n",
    "        \n",
    "        output = torch.stack(max_Q)\n",
    "        self.root.reset()\n",
    "        \n",
    "        return(-total_loss + total_C, output)\n",
    "    \n",
    "    \n",
    "    def collect_params(self):\n",
    "        self.nn_module_list = nn.ModuleList()\n",
    "        self.nn_param_list = nn.ParameterList()\n",
    "        mod, params = self.root.collect_params()\n",
    "        self.nn_module_list.extend(mod)\n",
    "        self.nn_param_list.extend(params)\n",
    "        \n",
    "        \n",
    "    def train_(self, train_loader, epoch):\n",
    "        for epoch_idx in range(1, epoch+1):\n",
    "            self.train()\n",
    "            self.initialize()\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                correct = 0\n",
    "                if self.cuda:\n",
    "                    data, target = data.cuda()\n",
    "                    \n",
    "                # if the batch size doesn't match (uneven division)\n",
    "                if not y_test.shape[0] == self.batch_size:\n",
    "                    self.initialize(batch_size)\n",
    "                    \n",
    "                data = data.view(self.batch_size, -1)\n",
    "                data = Variable(data)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                loss, output = self.calc_loss(data, target)\n",
    "                loss.backward(retain_variables=True)\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                pred = output.data.max(1)[1]\n",
    "\n",
    "                correct += pred.eq(target).cpu().sum()\n",
    "                accuracy = 100. * correct / len(data)\n",
    "\n",
    "                if batch_idx % self.log_interval == 0:\n",
    "\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accuracy: {}/{} ({:.4f}%)'.format(\n",
    "                        epoch_idx, batch_idx * len(data), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader), loss.data[0],\n",
    "                        correct, len(data),\n",
    "                        accuracy))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bz = 64\n",
    "input_dim = 28*28\n",
    "no_classes = 10\n",
    "max_depth = 2\n",
    "epochs = 4 \n",
    "lr = 0.01\n",
    "lmbda = 0.1\n",
    "momentum = 0.5\n",
    "seed = 1\n",
    "cuda = False\n",
    "log_interval = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory ./data already exists\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "try:\n",
    "    os.makedirs('./data')\n",
    "except:\n",
    "    print('directory ./data already exists')\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=bz, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=bz, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SoftDTree(bz, input_dim, no_classes, max_depth, epochs, lr, lmbda, momentum, seed, False, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timlee/anaconda2/envs/py3/lib/python3.6/site-packages/torch/autograd/__init__.py:93: UserWarning: retain_variables option is deprecated and will be removed in 0.3. Use retain_graph instead.\n",
      "  warnings.warn(\"retain_variables option is deprecated and will be removed in 0.3. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.737816, Accuracy: 4/64 (6.2500%)\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.578944, Accuracy: 12/64 (18.7500%)\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.548581, Accuracy: 9/64 (14.0625%)\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.450201, Accuracy: 12/64 (18.7500%)\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.416899, Accuracy: 13/64 (20.3125%)\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.431834, Accuracy: 14/64 (21.8750%)\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.316129, Accuracy: 17/64 (26.5625%)\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.211590, Accuracy: 18/64 (28.1250%)\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.204119, Accuracy: 21/64 (32.8125%)\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.233999, Accuracy: 17/64 (26.5625%)\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.104040, Accuracy: 20/64 (31.2500%)\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.126720, Accuracy: 19/64 (29.6875%)\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.094691, Accuracy: 17/64 (26.5625%)\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.135120, Accuracy: 14/64 (21.8750%)\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.104371, Accuracy: 15/64 (23.4375%)\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.107537, Accuracy: 15/64 (23.4375%)\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.016597, Accuracy: 22/64 (34.3750%)\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.871594, Accuracy: 24/64 (37.5000%)\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.081375, Accuracy: 16/64 (25.0000%)\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.981460, Accuracy: 18/64 (28.1250%)\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.033952, Accuracy: 15/64 (23.4375%)\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.055938, Accuracy: 17/64 (26.5625%)\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.993253, Accuracy: 21/64 (32.8125%)\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.006296, Accuracy: 17/64 (26.5625%)\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.971646, Accuracy: 19/64 (29.6875%)\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.957427, Accuracy: 18/64 (28.1250%)\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.061230, Accuracy: 15/64 (23.4375%)\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.001138, Accuracy: 13/64 (20.3125%)\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.040710, Accuracy: 21/64 (32.8125%)\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 1.884540, Accuracy: 22/64 (34.3750%)\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.936277, Accuracy: 22/64 (34.3750%)\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 1.917233, Accuracy: 18/64 (28.1250%)\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.942579, Accuracy: 23/64 (35.9375%)\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.918958, Accuracy: 19/64 (29.6875%)\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 1.915990, Accuracy: 16/64 (25.0000%)\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 1.953320, Accuracy: 19/64 (29.6875%)\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.796902, Accuracy: 27/64 (42.1875%)\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 1.826394, Accuracy: 20/64 (31.2500%)\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 1.829729, Accuracy: 25/64 (39.0625%)\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 1.772252, Accuracy: 29/64 (45.3125%)\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.898084, Accuracy: 21/64 (32.8125%)\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 1.879928, Accuracy: 22/64 (34.3750%)\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 1.797256, Accuracy: 28/64 (43.7500%)\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 1.968848, Accuracy: 14/64 (21.8750%)\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.824224, Accuracy: 21/64 (32.8125%)\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 1.950241, Accuracy: 19/64 (29.6875%)\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.884104, Accuracy: 22/64 (34.3750%)\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 1.913266, Accuracy: 16/64 (25.0000%)\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.899702, Accuracy: 17/64 (26.5625%)\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 1.839678, Accuracy: 22/64 (34.3750%)\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.881397, Accuracy: 17/64 (26.5625%)\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 1.761133, Accuracy: 24/64 (37.5000%)\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 1.796946, Accuracy: 24/64 (37.5000%)\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 1.770340, Accuracy: 21/64 (32.8125%)\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 1.988675, Accuracy: 17/64 (26.5625%)\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.891690, Accuracy: 21/64 (32.8125%)\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 1.829184, Accuracy: 25/64 (39.0625%)\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 1.744061, Accuracy: 27/64 (42.1875%)\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 1.791706, Accuracy: 22/64 (34.3750%)\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 1.815486, Accuracy: 22/64 (34.3750%)\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.761986, Accuracy: 21/64 (32.8125%)\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 1.783353, Accuracy: 25/64 (39.0625%)\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 1.586599, Accuracy: 32/64 (50.0000%)\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 1.844934, Accuracy: 29/64 (45.3125%)\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.812505, Accuracy: 22/64 (34.3750%)\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 1.887005, Accuracy: 22/64 (34.3750%)\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 1.734395, Accuracy: 28/64 (43.7500%)\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 1.860001, Accuracy: 21/64 (32.8125%)\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 1.769152, Accuracy: 24/64 (37.5000%)\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 1.749329, Accuracy: 31/64 (48.4375%)\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.722524, Accuracy: 28/64 (43.7500%)\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 1.618232, Accuracy: 32/64 (50.0000%)\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 1.696726, Accuracy: 30/64 (46.8750%)\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 1.675673, Accuracy: 26/64 (40.6250%)\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 1.727012, Accuracy: 29/64 (45.3125%)\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.946677, Accuracy: 18/64 (28.1250%)\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 1.842616, Accuracy: 20/64 (31.2500%)\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 1.940147, Accuracy: 17/64 (26.5625%)\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 1.719819, Accuracy: 23/64 (35.9375%)\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 1.800352, Accuracy: 23/64 (35.9375%)\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.757612, Accuracy: 28/64 (43.7500%)\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 1.742433, Accuracy: 27/64 (42.1875%)\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.830493, Accuracy: 22/64 (34.3750%)\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 1.761753, Accuracy: 26/64 (40.6250%)\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.714403, Accuracy: 27/64 (42.1875%)\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 1.683361, Accuracy: 26/64 (40.6250%)\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 1.723826, Accuracy: 23/64 (35.9375%)\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 1.713844, Accuracy: 28/64 (43.7500%)\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 1.740163, Accuracy: 23/64 (35.9375%)\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 1.643090, Accuracy: 30/64 (46.8750%)\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.764547, Accuracy: 23/64 (35.9375%)\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 1.709181, Accuracy: 26/64 (40.6250%)\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 1.712263, Accuracy: 23/64 (35.9375%)\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 1.636168, Accuracy: 30/64 (46.8750%)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [64 x 392], m2: [784 x 1] at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1416",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-225-337a7e60d278>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-221-3b6bedfa227f>\u001b[0m in \u001b[0;36mtrain_\u001b[0;34m(self, train_loader, epoch)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-221-3b6bedfa227f>\u001b[0m in \u001b[0;36mcalc_loss\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mall_leaf_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_prob_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# based on the path (max prob), get the distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-171-00cb40cef96b>\u001b[0m in \u001b[0;36mcalc_prob\u001b[0;34m(self, x, path_prob)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalc_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# calculate the inner probability with sigmoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# store the current path probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-171-00cb40cef96b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# this is the branch probability calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalc_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [64 x 392], m2: [784 x 1] at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1416"
     ]
    }
   ],
   "source": [
    "model.train_(train_loader, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_leaf_probs = model.root.calc_prob(x_var, model.path_prob_init)\n",
    "# loss = 0.\n",
    "# list_ = []\n",
    "# for path_prob, Q in all_leaf_probs:\n",
    "#     #print(path_prob.shape, Q.shape)\n",
    "#     #print(path_prob)\n",
    "#     #print(model.node_loss(y_var, Q, path_prob))\n",
    "#     loss += model.node_loss(y_var, Q, path_prob)\n",
    "#     list_.append(model.node_loss(y_var, Q, path_prob))\n",
    "# print(torch.sum(torch.stack(list_),dim=0))\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.calc_loss(x_var, y_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timlee/anaconda2/envs/py3/lib/python3.6/site-packages/torch/autograd/__init__.py:93: UserWarning: retain_variables option is deprecated and will be removed in 0.3. Use retain_graph instead.\n",
      "  warnings.warn(\"retain_variables option is deprecated and will be removed in 0.3. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/33604 (0%)]\tLoss: 3.039112, Accuracy: 7/64 (10.9375%)\n",
      "Train Epoch: 1 [640/33604 (2%)]\tLoss: 2.904632, Accuracy: 7/64 (10.9375%)\n",
      "Train Epoch: 1 [1280/33604 (4%)]\tLoss: 2.898018, Accuracy: 9/64 (14.0625%)\n",
      "Train Epoch: 1 [1920/33604 (6%)]\tLoss: 2.696019, Accuracy: 10/64 (15.6250%)\n",
      "Train Epoch: 1 [2560/33604 (8%)]\tLoss: 2.728435, Accuracy: 10/64 (15.6250%)\n",
      "Train Epoch: 1 [3200/33604 (10%)]\tLoss: 2.658377, Accuracy: 15/64 (23.4375%)\n",
      "Train Epoch: 1 [3840/33604 (11%)]\tLoss: 2.593014, Accuracy: 11/64 (17.1875%)\n",
      "Train Epoch: 1 [4480/33604 (13%)]\tLoss: 2.738613, Accuracy: 7/64 (10.9375%)\n",
      "Train Epoch: 1 [5120/33604 (15%)]\tLoss: 2.588169, Accuracy: 10/64 (15.6250%)\n",
      "Train Epoch: 1 [5760/33604 (17%)]\tLoss: 2.641657, Accuracy: 6/64 (9.3750%)\n",
      "Train Epoch: 1 [6400/33604 (19%)]\tLoss: 2.674805, Accuracy: 9/64 (14.0625%)\n",
      "Train Epoch: 1 [7040/33604 (21%)]\tLoss: 2.519283, Accuracy: 14/64 (21.8750%)\n",
      "Train Epoch: 1 [7680/33604 (23%)]\tLoss: 2.618356, Accuracy: 13/64 (20.3125%)\n",
      "Train Epoch: 1 [8320/33604 (25%)]\tLoss: 2.533704, Accuracy: 11/64 (17.1875%)\n",
      "Train Epoch: 1 [8960/33604 (27%)]\tLoss: 2.406267, Accuracy: 9/64 (14.0625%)\n",
      "Train Epoch: 1 [9600/33604 (29%)]\tLoss: 2.571110, Accuracy: 6/64 (9.3750%)\n",
      "Train Epoch: 1 [10240/33604 (30%)]\tLoss: 2.575189, Accuracy: 9/64 (14.0625%)\n",
      "Train Epoch: 1 [10880/33604 (32%)]\tLoss: 2.591006, Accuracy: 11/64 (17.1875%)\n",
      "Train Epoch: 1 [11520/33604 (34%)]\tLoss: 2.583909, Accuracy: 11/64 (17.1875%)\n",
      "Train Epoch: 1 [12160/33604 (36%)]\tLoss: 2.394065, Accuracy: 9/64 (14.0625%)\n",
      "Train Epoch: 1 [12800/33604 (38%)]\tLoss: 2.582700, Accuracy: 7/64 (10.9375%)\n",
      "Train Epoch: 1 [13440/33604 (40%)]\tLoss: 2.290311, Accuracy: 22/64 (34.3750%)\n",
      "Train Epoch: 1 [14080/33604 (42%)]\tLoss: 2.569142, Accuracy: 10/64 (15.6250%)\n",
      "Train Epoch: 1 [14720/33604 (44%)]\tLoss: 2.597366, Accuracy: 6/64 (9.3750%)\n",
      "Train Epoch: 1 [15360/33604 (46%)]\tLoss: 2.285976, Accuracy: 15/64 (23.4375%)\n",
      "Train Epoch: 1 [16000/33604 (48%)]\tLoss: 2.699528, Accuracy: 4/64 (6.2500%)\n",
      "Train Epoch: 1 [16640/33604 (49%)]\tLoss: 2.378452, Accuracy: 17/64 (26.5625%)\n",
      "Train Epoch: 1 [17280/33604 (51%)]\tLoss: 2.407158, Accuracy: 13/64 (20.3125%)\n",
      "Train Epoch: 1 [17920/33604 (53%)]\tLoss: 2.495213, Accuracy: 9/64 (14.0625%)\n",
      "Train Epoch: 1 [18560/33604 (55%)]\tLoss: 2.235303, Accuracy: 20/64 (31.2500%)\n",
      "Train Epoch: 1 [19200/33604 (57%)]\tLoss: 2.477613, Accuracy: 10/64 (15.6250%)\n",
      "Train Epoch: 1 [19840/33604 (59%)]\tLoss: 2.486878, Accuracy: 8/64 (12.5000%)\n",
      "Train Epoch: 1 [20480/33604 (61%)]\tLoss: 2.393239, Accuracy: 13/64 (20.3125%)\n",
      "Train Epoch: 1 [21120/33604 (63%)]\tLoss: 2.518937, Accuracy: 8/64 (12.5000%)\n",
      "Train Epoch: 1 [21760/33604 (65%)]\tLoss: 2.284273, Accuracy: 13/64 (20.3125%)\n",
      "Train Epoch: 1 [22400/33604 (67%)]\tLoss: 2.463883, Accuracy: 9/64 (14.0625%)\n",
      "Train Epoch: 1 [23040/33604 (68%)]\tLoss: 2.420129, Accuracy: 11/64 (17.1875%)\n",
      "Train Epoch: 1 [23680/33604 (70%)]\tLoss: 2.447800, Accuracy: 6/64 (9.3750%)\n",
      "Train Epoch: 1 [24320/33604 (72%)]\tLoss: 2.331107, Accuracy: 15/64 (23.4375%)\n",
      "Train Epoch: 1 [24960/33604 (74%)]\tLoss: 2.250690, Accuracy: 13/64 (20.3125%)\n",
      "Train Epoch: 1 [25600/33604 (76%)]\tLoss: 2.350848, Accuracy: 12/64 (18.7500%)\n",
      "Train Epoch: 1 [26240/33604 (78%)]\tLoss: 2.382120, Accuracy: 11/64 (17.1875%)\n",
      "Train Epoch: 1 [26880/33604 (80%)]\tLoss: 2.390853, Accuracy: 9/64 (14.0625%)\n",
      "Train Epoch: 1 [27520/33604 (82%)]\tLoss: 2.530230, Accuracy: 12/64 (18.7500%)\n",
      "Train Epoch: 1 [28160/33604 (84%)]\tLoss: 2.351905, Accuracy: 10/64 (15.6250%)\n",
      "Train Epoch: 1 [28800/33604 (86%)]\tLoss: 2.299152, Accuracy: 10/64 (15.6250%)\n",
      "Train Epoch: 1 [29440/33604 (87%)]\tLoss: 2.332526, Accuracy: 13/64 (20.3125%)\n",
      "Train Epoch: 1 [30080/33604 (89%)]\tLoss: 2.434921, Accuracy: 11/64 (17.1875%)\n",
      "Train Epoch: 1 [30720/33604 (91%)]\tLoss: 2.189824, Accuracy: 13/64 (20.3125%)\n",
      "Train Epoch: 1 [31360/33604 (93%)]\tLoss: 2.432451, Accuracy: 9/64 (14.0625%)\n",
      "Train Epoch: 1 [32000/33604 (95%)]\tLoss: 2.251378, Accuracy: 11/64 (17.1875%)\n",
      "Train Epoch: 1 [32640/33604 (97%)]\tLoss: 2.376524, Accuracy: 10/64 (15.6250%)\n",
      "Train Epoch: 1 [33280/33604 (99%)]\tLoss: 2.192362, Accuracy: 15/64 (23.4375%)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [64 x 49], m2: [784 x 1] at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1416",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-b1d1dc0032ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-207-3b6bedfa227f>\u001b[0m in \u001b[0;36mtrain_\u001b[0;34m(self, train_loader, epoch)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-207-3b6bedfa227f>\u001b[0m in \u001b[0;36mcalc_loss\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mall_leaf_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_prob_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# based on the path (max prob), get the distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-171-00cb40cef96b>\u001b[0m in \u001b[0;36mcalc_prob\u001b[0;34m(self, x, path_prob)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalc_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# calculate the inner probability with sigmoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# store the current path probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-171-00cb40cef96b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# this is the branch probability calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalc_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [64 x 49], m2: [784 x 1] at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1416"
     ]
    }
   ],
   "source": [
    "model.train_(train_loader, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
