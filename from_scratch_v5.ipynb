{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = datasets.MNIST('data', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "test_ds = datasets.MNIST('data', train=False, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatMNIST(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.n = len(self.dataset)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx]\n",
    "        return x.view(28*28), y\n",
    "\n",
    "    def __len__(self): return self.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ds = FlatMNIST(train_ds)\n",
    "ts_ds = FlatMNIST(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "#batch_size = 5 # for testing\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "if cuda:\n",
    "    train_loader = torch.utils.data.DataLoader(tr_ds, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(ts_ds, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "else:\n",
    "    train_loader = torch.utils.data.DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(ts_ds, batch_size=batch_size, shuffle=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTree2(nn.Module):\n",
    "    def __init__(self, tree_depth=2, n_classes=10, ni=28*28, lmbda = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_leaves = 2**tree_depth\n",
    "        self.n_classes = n_classes\n",
    "        self.num_nodes = self.num_leaves -1\n",
    "        self.tree_depth = tree_depth\n",
    "        \n",
    "        # regularization\n",
    "        self.lmbda = lmbda\n",
    "        self.nodes =  nn.ModuleList([nn.Linear(ni, 1) for i in range(self.num_nodes)])\n",
    "        self.leaves = nn.ParameterList([nn.Parameter(torch.randn(self.n_classes)) for i in range(self.num_leaves)])\n",
    "        \n",
    "        # inverse temperature filter\n",
    "        self.betas = nn.ParameterList([nn.Parameter(torch.randn(1)) for i in range(self.num_nodes)])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        bz = x.size()[0]\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "        # create distributions at each leaf - store for later\n",
    "        leaf_pcts = [softmax(leaf_param) for leaf_param in self.leaves]\n",
    "        leaf_dist = [pct.expand(bz, self.n_classes) for pct in leaf_pcts]\n",
    "        self.leaf_dist = leaf_dist\n",
    "        \n",
    "        # probabilities for inner nodes\n",
    "        p_x = [sigmoid(self.betas[i]*self.nodes[i](x)) for i in range(self.num_nodes)]\n",
    "    \n",
    "        # manually doing 2 tiers\n",
    "        path_prob = [p_x[0]*p_x[1], p_x[0]*(1-p_x[1]), (1-p_x[0])*p_x[2], (1-p_x[0])*(1-p_x[2])]\n",
    "        \n",
    "        return leaf_dist, path_prob, p_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tmp_mod(\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class tmp_mod(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = 1\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "    \n",
    "tp = tmp_mod()\n",
    "tp.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTree3(nn.Module):\n",
    "    def __init__(self, tree_depth=3, n_classes=10, ni=28*28, lmbda = 0.1, on_cuda=False):\n",
    "        super(NTree3,self).__init__()\n",
    "        self.num_leaves = 2**tree_depth\n",
    "        self.n_classes = n_classes\n",
    "        self.num_nodes = self.num_leaves -1\n",
    "        self.tree_depth = tree_depth\n",
    "        self.on_cuda = on_cuda\n",
    "        \n",
    "        # regularization\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "        \n",
    "        leaf_params = [torch.randn(self.n_classes) for i in range(self.num_leaves)]\n",
    "        beta_params = [torch.randn(1) for i in range(self.num_nodes)]\n",
    "                \n",
    "        if self.on_cuda==True:\n",
    "            beta_params = [beta_param.cuda() for beta_param in beta_params]\n",
    "            leaf_params = [leaf_param.cuda() for leaf_param in leaf_params]\n",
    "        \n",
    "        self.nodes =  nn.ModuleList([nn.Linear(ni, 1) for i in range(self.num_nodes)])\n",
    "        self.leaves = nn.ParameterList([nn.Parameter(leaf_param) for leaf_param in leaf_params])\n",
    "        \n",
    "        # inverse temperature filter\n",
    "        self.betas = nn.ParameterList([nn.Parameter(beta) for beta in beta_params])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        bz = x.size()[0]\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "        # create distributions at each leaf - store for later\n",
    "        leaf_pcts = [softmax(leaf_param) for leaf_param in self.leaves]\n",
    "        leaf_dist = [pct.expand(bz, self.n_classes) for pct in leaf_pcts]\n",
    "        self.leaf_dist = leaf_dist\n",
    "        \n",
    "        # probabilities of inner nodes\n",
    "        p_x = [sigmoid(self.betas[i]*self.nodes[i](x)) for i in range(self.num_nodes)]\n",
    "    \n",
    "        # manually doing 3 tiers\n",
    "        path_prob = [p_x[0]*p_x[1]*p_x[3],\n",
    "                     p_x[0]*p_x[1]*(1-p_x[3]),\n",
    "                     p_x[0]*(1-p_x[1])*p_x[4],\n",
    "                     p_x[0]*(1-p_x[1])*(1-p_x[4]),\n",
    "                     (1-p_x[0])*p_x[2]*p_x[5], \n",
    "                     (1-p_x[0])*p_x[2]*(1-p_x[5]),                      \n",
    "                     (1-p_x[0])*(1-p_x[2])*p_x[6],\n",
    "                     (1-p_x[0])*(1-p_x[2])*(1-p_x[6])                     \n",
    "                    ]\n",
    "        \n",
    "        return leaf_dist, path_prob, p_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigot_leaf_loss(path_prob, leaf_dist, labels, on_cuda):\n",
    "    ymask = torch.FloatTensor(leaf_dist.size()).zero_()\n",
    "    if on_cuda:\n",
    "        ymask = ymask.cuda()    \n",
    "    ymask.scatter_(1, labels.data.view(-1,1), 1)\n",
    "    ymask = Variable(ymask)\n",
    "    Tk_log_Qk = (torch.log(leaf_dist) * ymask).sum(1)\n",
    "    PTQ = Tk_log_Qk[:,None]*path_prob\n",
    "    return torch.sum(PTQ)\n",
    "\n",
    "\n",
    "def total_loss(path_probs, leaf_dists, labels, on_cuda=False):\n",
    "    L_x = [bigot_leaf_loss(path_prob, leaf_dist, labels, on_cuda) for path_prob, leaf_dist in zip(path_probs, leaf_dists)]\n",
    "    return(torch.log(-torch.sum(torch.stack(L_x))))\n",
    "\n",
    "\n",
    "def which_node(path_prob, n_leaves, on_cuda=False):\n",
    "    node_id = torch.max(torch.stack(path_prob),dim=0)[1]    \n",
    "    nodes_onehot = torch.FloatTensor(path_prob[0].size()[0], n_leaves).zero_()\n",
    "    if on_cuda:\n",
    "        node_id = node_id.cuda()\n",
    "        nodes_onehot = nodes_onehot.cuda()\n",
    "    node_mask = nodes_onehot.scatter_(1, node_id.data,1)\n",
    "    return(node_id,node_mask)\n",
    "\n",
    "\n",
    "def which_class(path_prob, leaf_dist, on_cuda=False):\n",
    "    n_leaves = len(leaf_dist)\n",
    "    node_id, node_mask = which_node(path_prob, n_leaves, on_cuda)\n",
    "    max_class_per_node = torch.t(torch.max(torch.stack(leaf_dist),dim=2)[1])\n",
    "    pred_class = torch.sum(Variable(node_mask.long())*max_class_per_node,dim=1)\n",
    "    return(pred_class)\n",
    "\n",
    "\n",
    "def acc_calc(val_dl, model, on_cuda=False):\n",
    "    model.eval()\n",
    "    val_ = iter(val_dl)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    final_dist = 0\n",
    "    n_leaves = 8\n",
    "    for i, batch in enumerate(val_dl):\n",
    "        data, labels = batch\n",
    "        if on_cuda:\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "        data_var, labels_var = Variable(data), Variable(labels)        \n",
    "        leaf_dist, path_prob, p_nodes = model(data_var)\n",
    "        final_dist += which_node(path_prob, n_leaves = 8, on_cuda=True)[1].sum(0)\n",
    "        \n",
    "        preds = which_class(path_prob, leaf_dist, on_cuda=on_cuda)\n",
    "        match = labels.eq(preds.data)\n",
    "        correct += match.sum()\n",
    "        total += match.size()[0]\n",
    "    return(correct/total, correct, total, final_dist) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = iter(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try one batch of 64 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = train_dl.next()\n",
    "data_var, labels_var = Variable(data), Variable(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NTree2()\n",
    "model3 = NTree3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = iter(train_loader)\n",
    "data, labels = train_dl.next()\n",
    "data_var, labels_var = Variable(data), Variable(labels)\n",
    "leaf_dist, path_prob, p_nodes = model(data_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(leaf_dist)\n",
    "len(path_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 16\n",
       " 44\n",
       "  4\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "  0\n",
       "[torch.FloatTensor of size 8]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "which_node(path_prob,8)[1].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0 , 00000/60000,  L: 5.150, A: 0.111, dist [652, 543, 874, 4088, 395, 1441, 865, 1142]\n",
      "Ep: 0 , 12800/60000,  L: 4.960, A: 0.195, dist [2117, 0, 0, 5669, 890, 0, 2, 1322]\n",
      "Ep: 0 , 25600/60000,  L: 4.852, A: 0.276, dist [1752, 4, 401, 3164, 2425, 0, 371, 1883]\n",
      "Ep: 0 , 38400/60000,  L: 4.871, A: 0.336, dist [2066, 28, 1024, 2665, 1347, 0, 577, 2293]\n",
      "Ep: 0 , 51200/60000,  L: 4.843, A: 0.347, dist [2146, 50, 1110, 2496, 1276, 0, 553, 2369]\n",
      "Ep: 1 , 00000/60000,  L: 4.863, A: 0.350, dist [2096, 61, 1140, 2431, 1146, 0, 620, 2506]\n",
      "Ep: 1 , 12800/60000,  L: 4.723, A: 0.349, dist [2067, 92, 1125, 2216, 1182, 0, 654, 2664]\n",
      "Ep: 1 , 25600/60000,  L: 4.765, A: 0.354, dist [2135, 132, 1087, 2208, 1105, 0, 662, 2671]\n",
      "Ep: 1 , 38400/60000,  L: 4.756, A: 0.425, dist [2087, 144, 1060, 2069, 1128, 0, 684, 2828]\n",
      "Ep: 1 , 51200/60000,  L: 4.685, A: 0.433, dist [2167, 219, 1044, 2169, 1043, 0, 672, 2686]\n",
      "Ep: 2 , 00000/60000,  L: 4.668, A: 0.436, dist [2175, 256, 1025, 2169, 1039, 0, 689, 2647]\n",
      "Ep: 2 , 12800/60000,  L: 4.684, A: 0.524, dist [2132, 271, 1039, 2125, 1043, 0, 686, 2704]\n",
      "Ep: 2 , 25600/60000,  L: 4.684, A: 0.526, dist [2124, 326, 1015, 2080, 1052, 0, 694, 2709]\n",
      "Ep: 2 , 38400/60000,  L: 4.633, A: 0.529, dist [2114, 310, 1064, 2154, 1051, 0, 695, 2612]\n",
      "Ep: 2 , 51200/60000,  L: 4.582, A: 0.530, dist [2124, 377, 1008, 2071, 1030, 0, 727, 2663]\n",
      "Ep: 3 , 00000/60000,  L: 4.605, A: 0.531, dist [2081, 385, 1055, 2097, 1065, 0, 716, 2601]\n",
      "Ep: 3 , 12800/60000,  L: 4.717, A: 0.533, dist [2103, 439, 1054, 2176, 1021, 0, 702, 2505]\n",
      "Ep: 3 , 25600/60000,  L: 4.677, A: 0.532, dist [2039, 433, 1076, 2103, 1054, 0, 705, 2590]\n",
      "Ep: 3 , 38400/60000,  L: 4.572, A: 0.535, dist [2027, 516, 1059, 2119, 1044, 0, 734, 2501]\n",
      "Ep: 3 , 51200/60000,  L: 4.533, A: 0.536, dist [2115, 550, 1057, 2118, 1056, 0, 724, 2380]\n",
      "Ep: 4 , 00000/60000,  L: 4.519, A: 0.536, dist [2091, 540, 1067, 2065, 1023, 0, 738, 2476]\n",
      "Ep: 4 , 12800/60000,  L: 4.601, A: 0.537, dist [2076, 579, 1059, 2092, 1022, 0, 729, 2443]\n",
      "Ep: 4 , 25600/60000,  L: 4.527, A: 0.538, dist [2101, 589, 1066, 2088, 1020, 0, 733, 2403]\n",
      "Ep: 4 , 38400/60000,  L: 4.538, A: 0.579, dist [2101, 693, 1074, 2172, 1038, 0, 695, 2227]\n",
      "Ep: 4 , 51200/60000,  L: 4.499, A: 0.579, dist [2058, 660, 1072, 2103, 1045, 0, 733, 2329]\n",
      "Ep: 5 , 00000/60000,  L: 4.520, A: 0.580, dist [2065, 669, 1069, 2103, 1041, 0, 748, 2305]\n",
      "Ep: 5 , 12800/60000,  L: 4.494, A: 0.584, dist [2053, 711, 1079, 2078, 1056, 0, 746, 2277]\n",
      "Ep: 5 , 25600/60000,  L: 4.529, A: 0.586, dist [2045, 740, 1070, 2089, 1043, 0, 748, 2265]\n",
      "Ep: 5 , 38400/60000,  L: 4.545, A: 0.588, dist [2090, 753, 1064, 2077, 1040, 0, 741, 2235]\n",
      "Ep: 5 , 51200/60000,  L: 4.585, A: 0.592, dist [2059, 740, 1104, 2160, 1024, 0, 743, 2170]\n",
      "Ep: 6 , 00000/60000,  L: 4.449, A: 0.590, dist [2114, 794, 1052, 2091, 1053, 0, 708, 2188]\n",
      "Ep: 6 , 12800/60000,  L: 4.455, A: 0.594, dist [2074, 766, 1087, 2115, 1034, 0, 721, 2203]\n",
      "Ep: 6 , 25600/60000,  L: 4.417, A: 0.593, dist [2042, 770, 1071, 2080, 1040, 0, 769, 2228]\n",
      "Ep: 6 , 38400/60000,  L: 4.387, A: 0.597, dist [2056, 810, 1087, 2115, 1020, 0, 729, 2183]\n",
      "Ep: 6 , 51200/60000,  L: 4.351, A: 0.599, dist [2085, 822, 1086, 2120, 1027, 0, 730, 2130]\n",
      "Ep: 7 , 00000/60000,  L: 4.366, A: 0.598, dist [2104, 809, 1079, 2088, 1027, 0, 733, 2160]\n",
      "Ep: 7 , 12800/60000,  L: 4.521, A: 0.600, dist [2083, 838, 1084, 2121, 1013, 0, 711, 2150]\n",
      "Ep: 7 , 25600/60000,  L: 4.343, A: 0.600, dist [2047, 817, 1085, 2097, 1033, 0, 731, 2190]\n",
      "Ep: 7 , 38400/60000,  L: 4.444, A: 0.601, dist [2063, 823, 1086, 2083, 1062, 0, 756, 2127]\n",
      "Ep: 7 , 51200/60000,  L: 4.435, A: 0.603, dist [2069, 857, 1085, 2084, 1029, 0, 735, 2141]\n",
      "Ep: 8 , 00000/60000,  L: 4.376, A: 0.603, dist [2028, 871, 1080, 2078, 1026, 0, 753, 2164]\n",
      "Ep: 8 , 12800/60000,  L: 4.263, A: 0.604, dist [2073, 833, 1096, 2105, 1042, 0, 741, 2110]\n",
      "Ep: 8 , 25600/60000,  L: 4.272, A: 0.604, dist [2051, 883, 1092, 2103, 1032, 0, 728, 2111]\n",
      "Ep: 8 , 38400/60000,  L: 4.407, A: 0.603, dist [2046, 828, 1089, 2079, 1035, 0, 756, 2167]\n",
      "Ep: 8 , 51200/60000,  L: 4.445, A: 0.606, dist [2064, 835, 1108, 2104, 1050, 0, 748, 2091]\n",
      "Ep: 9 , 00000/60000,  L: 4.425, A: 0.605, dist [2041, 838, 1095, 2073, 1056, 0, 769, 2128]\n",
      "Ep: 9 , 12800/60000,  L: 4.333, A: 0.607, dist [2052, 840, 1095, 2098, 1050, 0, 766, 2099]\n",
      "Ep: 9 , 25600/60000,  L: 4.336, A: 0.608, dist [2049, 873, 1099, 2090, 1042, 0, 765, 2082]\n",
      "Ep: 9 , 38400/60000,  L: 4.409, A: 0.608, dist [2036, 834, 1113, 2113, 1022, 0, 765, 2117]\n",
      "Ep: 9 , 51200/60000,  L: 4.420, A: 0.609, dist [2057, 890, 1109, 2121, 1024, 0, 747, 2052]\n",
      "Ep: 10 , 00000/60000,  L: 4.307, A: 0.607, dist [2015, 860, 1100, 2064, 1047, 0, 778, 2136]\n",
      "Ep: 10 , 12800/60000,  L: 4.319, A: 0.608, dist [2049, 835, 1119, 2109, 1022, 0, 774, 2092]\n",
      "Ep: 10 , 25600/60000,  L: 4.241, A: 0.607, dist [2049, 831, 1108, 2092, 1049, 0, 757, 2114]\n",
      "Ep: 10 , 38400/60000,  L: 4.381, A: 0.610, dist [2071, 887, 1097, 2102, 1032, 0, 764, 2047]\n",
      "Ep: 10 , 51200/60000,  L: 4.265, A: 0.611, dist [2035, 865, 1105, 2115, 1037, 0, 768, 2075]\n",
      "Ep: 11 , 00000/60000,  L: 4.377, A: 0.611, dist [2023, 849, 1110, 2095, 1051, 0, 782, 2090]\n",
      "Ep: 11 , 12800/60000,  L: 4.372, A: 0.610, dist [2036, 855, 1113, 2090, 1044, 0, 783, 2079]\n",
      "Ep: 11 , 25600/60000,  L: 4.408, A: 0.611, dist [2055, 858, 1118, 2116, 1039, 0, 754, 2060]\n",
      "Ep: 11 , 38400/60000,  L: 4.236, A: 0.610, dist [2066, 871, 1101, 2074, 1017, 0, 793, 2078]\n",
      "Ep: 11 , 51200/60000,  L: 4.429, A: 0.612, dist [2042, 862, 1111, 2094, 1041, 0, 788, 2062]\n",
      "Ep: 12 , 00000/60000,  L: 4.219, A: 0.611, dist [2006, 898, 1103, 2096, 1051, 0, 775, 2071]\n",
      "Ep: 12 , 12800/60000,  L: 4.204, A: 0.611, dist [2030, 856, 1107, 2064, 1051, 0, 794, 2098]\n",
      "Ep: 12 , 25600/60000,  L: 4.078, A: 0.611, dist [2026, 866, 1105, 2074, 1051, 0, 803, 2075]\n",
      "Ep: 12 , 38400/60000,  L: 4.246, A: 0.611, dist [2038, 849, 1114, 2073, 1051, 0, 789, 2086]\n",
      "Ep: 12 , 51200/60000,  L: 4.145, A: 0.613, dist [2038, 867, 1110, 2114, 1038, 0, 792, 2041]\n",
      "Ep: 13 , 00000/60000,  L: 4.463, A: 0.612, dist [2055, 891, 1097, 2077, 1026, 0, 801, 2053]\n",
      "Ep: 13 , 12800/60000,  L: 4.258, A: 0.612, dist [2050, 873, 1113, 2084, 1035, 0, 789, 2056]\n",
      "Ep: 13 , 25600/60000,  L: 4.319, A: 0.614, dist [2062, 904, 1112, 2104, 1026, 0, 788, 2004]\n",
      "Ep: 13 , 38400/60000,  L: 4.404, A: 0.613, dist [2039, 874, 1114, 2094, 1033, 0, 793, 2053]\n",
      "Ep: 13 , 51200/60000,  L: 4.158, A: 0.612, dist [2045, 867, 1107, 2067, 1036, 0, 803, 2075]\n",
      "Ep: 14 , 00000/60000,  L: 4.232, A: 0.613, dist [2053, 895, 1105, 2080, 1035, 0, 789, 2043]\n",
      "Ep: 14 , 12800/60000,  L: 4.107, A: 0.613, dist [2057, 884, 1101, 2079, 1035, 0, 808, 2036]\n",
      "Ep: 14 , 25600/60000,  L: 4.332, A: 0.613, dist [2039, 860, 1119, 2095, 1033, 0, 804, 2050]\n",
      "Ep: 14 , 38400/60000,  L: 4.302, A: 0.613, dist [2013, 890, 1109, 2079, 1025, 0, 822, 2062]\n",
      "Ep: 14 , 51200/60000,  L: 4.198, A: 0.614, dist [2016, 879, 1116, 2094, 1047, 0, 804, 2044]\n"
     ]
    }
   ],
   "source": [
    "is_cuda = False\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "if is_cuda:\n",
    "    train_loader = torch.utils.data.DataLoader(tr_ds, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(ts_ds, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "else:\n",
    "    train_loader = torch.utils.data.DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(ts_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = NTree3(tree_depth=3, n_classes=10, ni=28*28, lmbda = 0.1, on_cuda=is_cuda)\n",
    "\n",
    "if is_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "n_epochs = 15\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_dl = iter(train_loader)\n",
    "final_dist = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    train_dl = iter(train_loader)\n",
    "    \n",
    "    for i, batch in enumerate(train_dl):\n",
    "        data, labels = batch\n",
    "        bz = data.size()[0]\n",
    "        if is_cuda:\n",
    "            data = data.cuda()\n",
    "            labels = labels.cuda()\n",
    "        data_var, labels_var = Variable(data), Variable(labels)\n",
    "       \n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        leaf_dist, path_prob, p_nodes = model(data_var)\n",
    "        \n",
    "        loss = total_loss(path_prob, leaf_dist, labels_var, on_cuda=is_cuda)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%200 == 0:\n",
    "            acc, correct, total, dist = acc_calc(test_loader, model, on_cuda=is_cuda)\n",
    "            print('Ep: %d , %05d/60000,  L: %.03f, A: %.03f, dist %s' % (epoch, i*bz ,loss.data[0], acc, list(dist.long().cpu().numpy())))\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.leaf_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, correct, total, dist = acc_calc(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "539"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 927,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7586, 0, 0, 2414]"
      ]
     },
     "execution_count": 936,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dist.long().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "-2.0772\n",
       "-0.6781\n",
       "-1.6316\n",
       "-1.6203\n",
       " 0.3809\n",
       "-1.5100\n",
       "-1.1012\n",
       " 1.9916\n",
       "-1.4977\n",
       " 1.8553\n",
       "[torch.FloatTensor of size 10]"
      ]
     },
     "execution_count": 914,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.leaves[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "-2.2890\n",
       " 2.0111\n",
       "-0.4270\n",
       " 0.0372\n",
       "-1.9340\n",
       " 0.5527\n",
       "-1.2689\n",
       "-0.9730\n",
       " 1.6722\n",
       "-1.1189\n",
       "[torch.FloatTensor of size 10]"
      ]
     },
     "execution_count": 915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.leaves[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
