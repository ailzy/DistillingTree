{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ds = datasets.MNIST('data', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "test_ds = datasets.MNIST('data', train=False, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FlatMNIST(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.n = len(self.dataset)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx]\n",
    "        return x.view(28*28), y\n",
    "\n",
    "    def __len__(self): return self.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_ds = FlatMNIST(train_ds)\n",
    "ts_ds = FlatMNIST(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "#batch_size = 5 # for testing\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "if cuda:\n",
    "    train_loader = torch.utils.data.DataLoader(tr_ds, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(ts_ds, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "else:\n",
    "    train_loader = torch.utils.data.DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(ts_ds, batch_size=batch_size, shuffle=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.3180\n",
       " 0.0370\n",
       " 0.1363\n",
       " 0.0285\n",
       " 0.0128\n",
       " 0.0388\n",
       " 0.1669\n",
       " 0.1886\n",
       " 0.0333\n",
       " 0.0397\n",
       "[torch.FloatTensor of size 10x1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(Variable(torch.randn(10,1)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTree2(nn.Module):\n",
    "    def __init__(self, tree_depth=2, n_classes=10, ni=28*28, lmbda = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_leaves = 2**tree_depth\n",
    "        self.n_classes = n_classes\n",
    "        self.num_nodes = self.num_leaves -1\n",
    "        self.tree_depth = tree_depth\n",
    "        \n",
    "        # regularization\n",
    "        self.lmbda = lmbda\n",
    "        self.nodes =  nn.ModuleList([nn.Linear(ni, 1) for i in range(self.num_nodes)])\n",
    "        self.leaves = nn.ParameterList([nn.Parameter(torch.randn(self.n_classes)) for i in range(self.num_leaves)])\n",
    "        \n",
    "        # inverse temperature filter\n",
    "        self.betas = nn.ParameterList([nn.Parameter(torch.randn(1)) for i in range(self.num_nodes)])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        bz = x.size()[0]\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "        # create distributions at each leaf - store for later\n",
    "        leaf_pcts = [softmax(leaf_param) for leaf_param in self.leaves]\n",
    "        leaf_dist = [pct.expand(bz, self.n_classes) for pct in leaf_pcts]\n",
    "        self.leaf_dist = leaf_dist\n",
    "        \n",
    "        # probabilities for inner nodes\n",
    "        p_x = [sigmoid(self.betas[i]*self.nodes[i](x)) for i in range(self.num_nodes)]\n",
    "    \n",
    "        # manually doing 2 tiers\n",
    "        path_prob = [p_x[0]*p_x[1], p_x[0]*(1-p_x[1]), (1-p_x[0])*p_x[2], (1-p_x[0])*(1-p_x[2])]\n",
    "        \n",
    "        return leaf_dist, path_prob, p_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NTree3(nn.Module):\n",
    "    def __init__(self, tree_depth=3, n_classes=10, ni=28*28, lmbda = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_leaves = 2**tree_depth\n",
    "        self.n_classes = n_classes\n",
    "        self.num_nodes = self.num_leaves -1\n",
    "        self.tree_depth = tree_depth\n",
    "        \n",
    "        # regularization\n",
    "        self.lmbda = lmbda\n",
    "        self.nodes =  nn.ModuleList([nn.Linear(ni, 1) for i in range(self.num_nodes)])\n",
    "        self.leaves = nn.ParameterList([nn.Parameter(torch.randn(self.n_classes)) for i in range(self.num_leaves)])\n",
    "        \n",
    "        # inverse temperature filter\n",
    "        self.betas = nn.ParameterList([nn.Parameter(torch.randn(1)) for i in range(self.num_nodes)])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        bz = x.size()[0]\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "        # create distributions at each leaf - store for later\n",
    "        leaf_pcts = [softmax(leaf_param) for leaf_param in self.leaves]\n",
    "        leaf_dist = [pct.expand(bz, self.n_classes) for pct in leaf_pcts]\n",
    "        self.leaf_dist = leaf_dist\n",
    "        \n",
    "        # probabilities for inner nodes\n",
    "        p_x = [sigmoid(self.betas[i]*self.nodes[i](x)) for i in range(self.num_nodes)]\n",
    "    \n",
    "        # manually doing 3 tiers\n",
    "        path_prob = [p_x[0]*p_x[1]*p_x[3],\n",
    "                     p_x[0]*p_x[1]*(1-p_x[3]),\n",
    "                     p_x[0]*(1-p_x[1])*p_x[4],\n",
    "                     p_x[0]*(1-p_x[1])*(1-p_x[4]),\n",
    "                     (1-p_x[0])*p_x[2]*p_x[5], \n",
    "                     (1-p_x[0])*p_x[2]*(1-p_x[5]),                      \n",
    "                     (1-p_x[0])*(1-p_x[2])*p_x[6],\n",
    "                     (1-p_x[0])*(1-p_x[2])*(1-p_x[6])                     \n",
    "                    ]\n",
    "        \n",
    "        return leaf_dist, path_prob, p_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigot_leaf_loss(path_prob, leaf_dist, labels):\n",
    "    ymask = torch.FloatTensor(leaf_dist.size()).zero_()\n",
    "    ymask.scatter_(1, labels.data.view(-1,1), 1)\n",
    "    ymask = Variable(ymask)\n",
    "    Tk_log_Qk = (torch.log(leaf_dist) * ymask).sum(1)\n",
    "    PTQ = Tk_log_Qk[:,None]*path_prob\n",
    "    return torch.sum(PTQ)\n",
    "\n",
    "\n",
    "def total_loss(path_probs, leaf_dists, labels):\n",
    "    L_x = [bigot_leaf_loss(path_prob, leaf_dist, labels) for path_prob, leaf_dist in zip(path_probs, leaf_dists)]\n",
    "    return(torch.log(-torch.sum(torch.stack(L_x))))\n",
    "\n",
    "\n",
    "def which_node(path_prob, n_leaves):\n",
    "    node_id = torch.max(torch.stack(path_prob),dim=0)[1]\n",
    "    nodes_onehot = torch.FloatTensor(path_prob[0].size()[0], n_leaves).zero_()\n",
    "    node_mask = nodes_onehot.scatter_(1, node_id.data,1)\n",
    "    return(node_id,node_mask)\n",
    "\n",
    "\n",
    "def which_class(path_prob, leaf_dist):\n",
    "    n_leaves = len(leaf_dist)\n",
    "    node_id, node_mask = which_node(path_prob, n_leaves)\n",
    "    max_class_per_node = torch.t(torch.max(torch.stack(leaf_dist),dim=2)[1])\n",
    "    pred_class = torch.sum(Variable(node_mask.long())*max_class_per_node,dim=1)\n",
    "    return(pred_class)\n",
    "\n",
    "\n",
    "def acc_calc(val_dl, model):\n",
    "    model.eval()\n",
    "    val_ = iter(val_dl)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    final_dist = 0\n",
    "    n_leaves = 8\n",
    "    for i, batch in enumerate(val_dl):\n",
    "        data, labels = batch\n",
    "        data_var, labels_var = Variable(data), Variable(labels)        \n",
    "        leaf_dist, path_prob, p_nodes = model(data_var)\n",
    "        final_dist += which_node(path_prob, n_leaves = 8)[1].sum(0)\n",
    "        \n",
    "        preds = which_class(path_prob, leaf_dist)\n",
    "        match = labels.eq(preds.data)\n",
    "        correct += match.sum()\n",
    "        total += match.size()[0]\n",
    "    return(correct/total, correct, total, final_dist) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dl = iter(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try one batch of 64 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = train_dl.next()\n",
    "data_var, labels_var = Variable(data), Variable(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = NTree2()\n",
    "model = NTree3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = iter(train_loader)\n",
    "data, labels = train_dl.next()\n",
    "data_var, labels_var = Variable(data), Variable(labels)\n",
    "leaf_dist, path_prob, p_nodes = model(data_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(leaf_dist)\n",
    "len(path_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Variable containing:\n",
       "  0.5243\n",
       "  0.7274\n",
       "  0.4359\n",
       "  0.6828\n",
       "  0.4383\n",
       "  0.6025\n",
       "  0.4270\n",
       "  0.5395\n",
       "  0.5404\n",
       "  0.3634\n",
       "  0.3062\n",
       "  0.4253\n",
       "  0.8027\n",
       "  0.4718\n",
       "  0.2910\n",
       "  0.4861\n",
       "  0.6226\n",
       "  0.3500\n",
       "  0.4169\n",
       "  0.4069\n",
       "  0.4033\n",
       "  0.4691\n",
       "  0.4266\n",
       "  0.5239\n",
       "  0.6072\n",
       "  0.6877\n",
       "  0.6229\n",
       "  0.4558\n",
       "  0.4724\n",
       "  0.6173\n",
       "  0.5637\n",
       "  0.6454\n",
       "  0.2830\n",
       "  0.4617\n",
       "  0.5207\n",
       "  0.4583\n",
       "  0.4228\n",
       "  0.3241\n",
       "  0.6258\n",
       "  0.4920\n",
       "  0.3861\n",
       "  0.2906\n",
       "  0.6850\n",
       "  0.5055\n",
       "  0.4674\n",
       "  0.5462\n",
       "  0.5243\n",
       "  0.3371\n",
       "  0.6368\n",
       "  0.6639\n",
       "  0.4933\n",
       "  0.4415\n",
       "  0.3350\n",
       "  0.5018\n",
       "  0.5203\n",
       "  0.4005\n",
       "  0.4713\n",
       "  0.4937\n",
       "  0.4421\n",
       "  0.6957\n",
       "  0.5062\n",
       "  0.6551\n",
       "  0.4072\n",
       "  0.6278\n",
       " [torch.FloatTensor of size 64x1], Variable containing:\n",
       "  0.4496\n",
       "  0.4299\n",
       "  0.7984\n",
       "  0.4764\n",
       "  0.6367\n",
       "  0.6180\n",
       "  0.3971\n",
       "  0.4953\n",
       "  0.6042\n",
       "  0.5410\n",
       "  0.5075\n",
       "  0.3589\n",
       "  0.6318\n",
       "  0.5393\n",
       "  0.6392\n",
       "  0.5596\n",
       "  0.7495\n",
       "  0.5330\n",
       "  0.8445\n",
       "  0.7054\n",
       "  0.8474\n",
       "  0.5503\n",
       "  0.6392\n",
       "  0.7086\n",
       "  0.7280\n",
       "  0.5346\n",
       "  0.5052\n",
       "  0.7178\n",
       "  0.5114\n",
       "  0.3825\n",
       "  0.4774\n",
       "  0.4351\n",
       "  0.6000\n",
       "  0.5906\n",
       "  0.7340\n",
       "  0.7333\n",
       "  0.5567\n",
       "  0.4645\n",
       "  0.5961\n",
       "  0.7124\n",
       "  0.5586\n",
       "  0.5796\n",
       "  0.5300\n",
       "  0.6499\n",
       "  0.5123\n",
       "  0.7163\n",
       "  0.6421\n",
       "  0.7223\n",
       "  0.4681\n",
       "  0.4849\n",
       "  0.5392\n",
       "  0.4090\n",
       "  0.7534\n",
       "  0.6822\n",
       "  0.4040\n",
       "  0.7473\n",
       "  0.6297\n",
       "  0.6832\n",
       "  0.7059\n",
       "  0.6023\n",
       "  0.6622\n",
       "  0.7722\n",
       "  0.7703\n",
       "  0.6096\n",
       " [torch.FloatTensor of size 64x1], Variable containing:\n",
       "  0.4633\n",
       "  0.5473\n",
       "  0.4468\n",
       "  0.4769\n",
       "  0.5595\n",
       "  0.5104\n",
       "  0.4997\n",
       "  0.5262\n",
       "  0.5351\n",
       "  0.5347\n",
       "  0.4523\n",
       "  0.4272\n",
       "  0.4491\n",
       "  0.5217\n",
       "  0.4651\n",
       "  0.4547\n",
       "  0.4978\n",
       "  0.4628\n",
       "  0.3787\n",
       "  0.4564\n",
       "  0.4637\n",
       "  0.4901\n",
       "  0.3926\n",
       "  0.4886\n",
       "  0.6125\n",
       "  0.5354\n",
       "  0.4739\n",
       "  0.4496\n",
       "  0.4216\n",
       "  0.5000\n",
       "  0.4977\n",
       "  0.4295\n",
       "  0.3942\n",
       "  0.4964\n",
       "  0.4708\n",
       "  0.5067\n",
       "  0.4662\n",
       "  0.4891\n",
       "  0.5788\n",
       "  0.4362\n",
       "  0.4358\n",
       "  0.4225\n",
       "  0.5365\n",
       "  0.5228\n",
       "  0.3918\n",
       "  0.4678\n",
       "  0.5062\n",
       "  0.4324\n",
       "  0.4369\n",
       "  0.4780\n",
       "  0.4765\n",
       "  0.5303\n",
       "  0.5189\n",
       "  0.4678\n",
       "  0.5578\n",
       "  0.4602\n",
       "  0.4549\n",
       "  0.4216\n",
       "  0.4345\n",
       "  0.4895\n",
       "  0.4187\n",
       "  0.4676\n",
       "  0.5060\n",
       "  0.5246\n",
       " [torch.FloatTensor of size 64x1], Variable containing:\n",
       "  0.3780\n",
       "  0.2880\n",
       "  0.2700\n",
       "  0.4501\n",
       "  0.4061\n",
       "  0.4448\n",
       "  0.3849\n",
       "  0.3625\n",
       "  0.3486\n",
       "  0.3618\n",
       "  0.2953\n",
       "  0.3237\n",
       "  0.3401\n",
       "  0.3877\n",
       "  0.3958\n",
       "  0.3141\n",
       "  0.3394\n",
       "  0.3354\n",
       "  0.4237\n",
       "  0.3667\n",
       "  0.2555\n",
       "  0.3173\n",
       "  0.4512\n",
       "  0.2516\n",
       "  0.3818\n",
       "  0.2811\n",
       "  0.3298\n",
       "  0.3112\n",
       "  0.4680\n",
       "  0.3416\n",
       "  0.3636\n",
       "  0.2205\n",
       "  0.4644\n",
       "  0.4201\n",
       "  0.4589\n",
       "  0.3810\n",
       "  0.2803\n",
       "  0.5042\n",
       "  0.3796\n",
       "  0.4349\n",
       "  0.4666\n",
       "  0.4662\n",
       "  0.3834\n",
       "  0.3559\n",
       "  0.4644\n",
       "  0.3896\n",
       "  0.2204\n",
       "  0.3955\n",
       "  0.2243\n",
       "  0.3095\n",
       "  0.3542\n",
       "  0.3657\n",
       "  0.3719\n",
       "  0.3484\n",
       "  0.2883\n",
       "  0.3205\n",
       "  0.5172\n",
       "  0.4915\n",
       "  0.4194\n",
       "  0.3378\n",
       "  0.2670\n",
       "  0.3709\n",
       "  0.3145\n",
       "  0.3240\n",
       " [torch.FloatTensor of size 64x1], Variable containing:\n",
       "  0.5260\n",
       "  0.4809\n",
       "  0.5048\n",
       "  0.5236\n",
       "  0.5314\n",
       "  0.4823\n",
       "  0.5197\n",
       "  0.5012\n",
       "  0.4699\n",
       "  0.4997\n",
       "  0.4896\n",
       "  0.4903\n",
       "  0.5335\n",
       "  0.5252\n",
       "  0.4634\n",
       "  0.5181\n",
       "  0.5025\n",
       "  0.5309\n",
       "  0.5064\n",
       "  0.4965\n",
       "  0.5239\n",
       "  0.4570\n",
       "  0.4836\n",
       "  0.4593\n",
       "  0.5181\n",
       "  0.5137\n",
       "  0.4991\n",
       "  0.5394\n",
       "  0.4760\n",
       "  0.4833\n",
       "  0.4882\n",
       "  0.5095\n",
       "  0.4841\n",
       "  0.4919\n",
       "  0.5145\n",
       "  0.4596\n",
       "  0.5068\n",
       "  0.4851\n",
       "  0.5079\n",
       "  0.4925\n",
       "  0.4587\n",
       "  0.4735\n",
       "  0.4708\n",
       "  0.4964\n",
       "  0.4780\n",
       "  0.5214\n",
       "  0.4674\n",
       "  0.4960\n",
       "  0.5092\n",
       "  0.4998\n",
       "  0.4928\n",
       "  0.4876\n",
       "  0.4892\n",
       "  0.4536\n",
       "  0.5209\n",
       "  0.4850\n",
       "  0.4763\n",
       "  0.4987\n",
       "  0.5196\n",
       "  0.4934\n",
       "  0.5058\n",
       "  0.4790\n",
       "  0.5001\n",
       "  0.5144\n",
       " [torch.FloatTensor of size 64x1], Variable containing:\n",
       "  0.4656\n",
       "  0.6352\n",
       "  0.3983\n",
       "  0.6689\n",
       "  0.4492\n",
       "  0.4192\n",
       "  0.4696\n",
       "  0.4899\n",
       "  0.5082\n",
       "  0.5849\n",
       "  0.4092\n",
       "  0.5732\n",
       "  0.5095\n",
       "  0.6529\n",
       "  0.5607\n",
       "  0.5079\n",
       "  0.5262\n",
       "  0.5272\n",
       "  0.4318\n",
       "  0.4162\n",
       "  0.3644\n",
       "  0.6604\n",
       "  0.3662\n",
       "  0.6658\n",
       "  0.4635\n",
       "  0.3774\n",
       "  0.4393\n",
       "  0.4179\n",
       "  0.4956\n",
       "  0.4210\n",
       "  0.4008\n",
       "  0.5104\n",
       "  0.4856\n",
       "  0.4524\n",
       "  0.5755\n",
       "  0.6435\n",
       "  0.6528\n",
       "  0.4605\n",
       "  0.4531\n",
       "  0.4326\n",
       "  0.6002\n",
       "  0.4853\n",
       "  0.5012\n",
       "  0.3598\n",
       "  0.5497\n",
       "  0.4718\n",
       "  0.6047\n",
       "  0.5263\n",
       "  0.5515\n",
       "  0.7448\n",
       "  0.7202\n",
       "  0.5632\n",
       "  0.5542\n",
       "  0.4971\n",
       "  0.5961\n",
       "  0.6592\n",
       "  0.4814\n",
       "  0.5806\n",
       "  0.3780\n",
       "  0.4322\n",
       "  0.5158\n",
       "  0.5455\n",
       "  0.5036\n",
       "  0.5907\n",
       " [torch.FloatTensor of size 64x1], Variable containing:\n",
       "  0.5053\n",
       "  0.5281\n",
       "  0.4592\n",
       "  0.5145\n",
       "  0.4817\n",
       "  0.4978\n",
       "  0.4743\n",
       "  0.4988\n",
       "  0.4798\n",
       "  0.4606\n",
       "  0.4418\n",
       "  0.4675\n",
       "  0.4941\n",
       "  0.5072\n",
       "  0.4098\n",
       "  0.4777\n",
       "  0.4630\n",
       "  0.4978\n",
       "  0.4397\n",
       "  0.4867\n",
       "  0.4274\n",
       "  0.4980\n",
       "  0.5149\n",
       "  0.4718\n",
       "  0.4611\n",
       "  0.4796\n",
       "  0.5607\n",
       "  0.4689\n",
       "  0.4749\n",
       "  0.4971\n",
       "  0.4809\n",
       "  0.4839\n",
       "  0.4829\n",
       "  0.4674\n",
       "  0.4795\n",
       "  0.4917\n",
       "  0.5272\n",
       "  0.4940\n",
       "  0.4428\n",
       "  0.4843\n",
       "  0.4720\n",
       "  0.4715\n",
       "  0.5048\n",
       "  0.4790\n",
       "  0.5010\n",
       "  0.4295\n",
       "  0.4769\n",
       "  0.4769\n",
       "  0.5550\n",
       "  0.5338\n",
       "  0.5110\n",
       "  0.4889\n",
       "  0.4696\n",
       "  0.4831\n",
       "  0.4862\n",
       "  0.4443\n",
       "  0.4940\n",
       "  0.4794\n",
       "  0.4619\n",
       "  0.5337\n",
       "  0.4321\n",
       "  0.4551\n",
       "  0.4635\n",
       "  0.4584\n",
       " [torch.FloatTensor of size 64x1]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  2\n",
       " 10\n",
       " 19\n",
       "  6\n",
       " 13\n",
       "  6\n",
       "  3\n",
       "  5\n",
       "[torch.FloatTensor of size 8]"
      ]
     },
     "execution_count": 979,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "which_node(path_prob,8)[1].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0 , 00000/60000,  L: 5.181, A: 0.094, dist [1253, 472, 161, 104, 2951, 2903, 1017, 1139]\n",
      "Ep: 0 , 12800/60000,  L: 5.020, A: 0.192, dist [116, 273, 0, 0, 2370, 0, 2521, 4720]\n",
      "Ep: 0 , 25600/60000,  L: 4.953, A: 0.195, dist [7, 7, 0, 0, 2439, 591, 3259, 3697]\n",
      "Ep: 0 , 38400/60000,  L: 4.860, A: 0.195, dist [0, 0, 0, 0, 2801, 1491, 3150, 2558]\n",
      "Ep: 0 , 51200/60000,  L: 4.861, A: 0.207, dist [0, 0, 0, 0, 2897, 1683, 3128, 2292]\n",
      "Ep: 1 , 00000/60000,  L: 4.814, A: 0.204, dist [0, 0, 0, 0, 2934, 1774, 3069, 2223]\n",
      "Ep: 1 , 12800/60000,  L: 4.839, A: 0.203, dist [0, 0, 0, 0, 2992, 1813, 2986, 2209]\n",
      "Ep: 1 , 25600/60000,  L: 4.844, A: 0.288, dist [0, 0, 0, 0, 3009, 1829, 3041, 2121]\n",
      "Ep: 1 , 38400/60000,  L: 4.778, A: 0.287, dist [0, 0, 0, 0, 3034, 1859, 2998, 2109]\n",
      "Ep: 1 , 51200/60000,  L: 4.747, A: 0.289, dist [0, 0, 0, 0, 2916, 1871, 3004, 2209]\n",
      "Ep: 2 , 00000/60000,  L: 4.790, A: 0.389, dist [0, 0, 0, 0, 3040, 1900, 2975, 2085]\n",
      "Ep: 2 , 12800/60000,  L: 4.764, A: 0.389, dist [0, 0, 0, 0, 3003, 1903, 2991, 2103]\n",
      "Ep: 2 , 25600/60000,  L: 4.823, A: 0.389, dist [0, 0, 0, 0, 2942, 1878, 2963, 2217]\n",
      "Ep: 2 , 38400/60000,  L: 4.666, A: 0.391, dist [0, 0, 0, 0, 2921, 1937, 2990, 2152]\n",
      "Ep: 2 , 51200/60000,  L: 4.731, A: 0.390, dist [0, 0, 0, 0, 2994, 1909, 2999, 2098]\n",
      "Ep: 3 , 00000/60000,  L: 4.729, A: 0.391, dist [0, 0, 0, 0, 3000, 1934, 2954, 2112]\n",
      "Ep: 3 , 12800/60000,  L: 4.600, A: 0.390, dist [0, 0, 0, 0, 2983, 1904, 2979, 2134]\n",
      "Ep: 3 , 25600/60000,  L: 4.722, A: 0.391, dist [0, 0, 0, 0, 2970, 1924, 2970, 2136]\n",
      "Ep: 3 , 38400/60000,  L: 4.679, A: 0.391, dist [0, 0, 0, 0, 2924, 1914, 2972, 2190]\n",
      "Ep: 3 , 51200/60000,  L: 4.768, A: 0.391, dist [0, 0, 0, 0, 2899, 1926, 2968, 2207]\n",
      "Ep: 4 , 00000/60000,  L: 4.738, A: 0.392, dist [0, 6, 0, 0, 2960, 1930, 2981, 2123]\n",
      "Ep: 4 , 12800/60000,  L: 4.666, A: 0.392, dist [0, 303, 0, 0, 2938, 1937, 2679, 2143]\n",
      "Ep: 4 , 25600/60000,  L: 4.688, A: 0.449, dist [0, 589, 0, 0, 2866, 1907, 2407, 2231]\n",
      "Ep: 4 , 38400/60000,  L: 4.733, A: 0.459, dist [0, 707, 0, 0, 2836, 1939, 2298, 2220]\n",
      "Ep: 4 , 51200/60000,  L: 4.664, A: 0.464, dist [0, 766, 0, 0, 2884, 1892, 2235, 2223]\n",
      "Ep: 5 , 00000/60000,  L: 4.678, A: 0.467, dist [0, 788, 0, 0, 2831, 1940, 2215, 2226]\n",
      "Ep: 5 , 12800/60000,  L: 4.580, A: 0.470, dist [0, 825, 0, 0, 2892, 1924, 2158, 2201]\n",
      "Ep: 5 , 25600/60000,  L: 4.530, A: 0.472, dist [2, 849, 0, 0, 2787, 1913, 2160, 2289]\n",
      "Ep: 5 , 38400/60000,  L: 4.620, A: 0.474, dist [2, 879, 0, 0, 2829, 1913, 2123, 2254]\n",
      "Ep: 5 , 51200/60000,  L: 4.611, A: 0.476, dist [3, 903, 0, 0, 2808, 1947, 2106, 2233]\n",
      "Ep: 6 , 00000/60000,  L: 4.541, A: 0.476, dist [3, 908, 0, 0, 2792, 1925, 2108, 2264]\n",
      "Ep: 6 , 12800/60000,  L: 4.536, A: 0.478, dist [5, 930, 0, 0, 2822, 1923, 2077, 2243]\n",
      "Ep: 6 , 25600/60000,  L: 4.585, A: 0.478, dist [10, 922, 0, 0, 2748, 1934, 2075, 2311]\n",
      "Ep: 6 , 38400/60000,  L: 4.627, A: 0.479, dist [9, 928, 0, 0, 2729, 1956, 2076, 2302]\n",
      "Ep: 6 , 51200/60000,  L: 4.504, A: 0.480, dist [9, 941, 0, 0, 2745, 1934, 2066, 2305]\n",
      "Ep: 7 , 00000/60000,  L: 4.532, A: 0.480, dist [10, 947, 0, 0, 2763, 1911, 2069, 2300]\n",
      "Ep: 7 , 12800/60000,  L: 4.581, A: 0.480, dist [11, 951, 0, 0, 2701, 1920, 2056, 2361]\n",
      "Ep: 7 , 25600/60000,  L: 4.541, A: 0.481, dist [13, 954, 0, 0, 2628, 1936, 2068, 2401]\n",
      "Ep: 7 , 38400/60000,  L: 4.554, A: 0.482, dist [15, 984, 0, 0, 2656, 1912, 2037, 2396]\n",
      "Ep: 7 , 51200/60000,  L: 4.502, A: 0.481, dist [16, 960, 0, 0, 2628, 1918, 2043, 2435]\n",
      "Ep: 8 , 00000/60000,  L: 4.505, A: 0.482, dist [15, 970, 0, 0, 2638, 1920, 2039, 2418]\n",
      "Ep: 8 , 12800/60000,  L: 4.663, A: 0.482, dist [15, 973, 0, 0, 2555, 1917, 2061, 2479]\n",
      "Ep: 8 , 25600/60000,  L: 4.553, A: 0.483, dist [23, 978, 0, 0, 2564, 1932, 2030, 2473]\n",
      "Ep: 8 , 38400/60000,  L: 4.469, A: 0.483, dist [24, 977, 0, 0, 2592, 1908, 2035, 2464]\n",
      "Ep: 8 , 51200/60000,  L: 4.470, A: 0.484, dist [25, 984, 0, 0, 2584, 1931, 2021, 2455]\n",
      "Ep: 9 , 00000/60000,  L: 4.480, A: 0.483, dist [24, 992, 0, 0, 2463, 1908, 2021, 2592]\n",
      "Ep: 9 , 12800/60000,  L: 4.482, A: 0.484, dist [23, 987, 0, 0, 2487, 1925, 2031, 2547]\n",
      "Ep: 9 , 25600/60000,  L: 4.445, A: 0.484, dist [26, 990, 0, 0, 2388, 1916, 2018, 2662]\n",
      "Ep: 9 , 38400/60000,  L: 4.488, A: 0.484, dist [26, 1004, 0, 0, 2429, 1903, 2012, 2626]\n",
      "Ep: 9 , 51200/60000,  L: 4.411, A: 0.484, dist [31, 986, 0, 0, 2377, 1914, 2017, 2675]\n",
      "Ep: 10 , 00000/60000,  L: 4.459, A: 0.485, dist [30, 1011, 0, 0, 2401, 1923, 1996, 2639]\n",
      "Ep: 10 , 12800/60000,  L: 4.480, A: 0.485, dist [32, 1001, 0, 0, 2389, 1917, 2001, 2660]\n",
      "Ep: 10 , 25600/60000,  L: 4.481, A: 0.485, dist [31, 1006, 0, 0, 2376, 1912, 1998, 2677]\n",
      "Ep: 10 , 38400/60000,  L: 4.413, A: 0.486, dist [32, 1015, 0, 0, 2270, 1920, 2011, 2752]\n",
      "Ep: 10 , 51200/60000,  L: 4.413, A: 0.485, dist [32, 1013, 0, 0, 2264, 1905, 2008, 2778]\n",
      "Ep: 11 , 00000/60000,  L: 4.390, A: 0.486, dist [34, 1020, 0, 0, 2279, 1911, 2000, 2756]\n",
      "Ep: 11 , 12800/60000,  L: 4.415, A: 0.486, dist [35, 1016, 0, 0, 2275, 1912, 1999, 2763]\n",
      "Ep: 11 , 25600/60000,  L: 4.392, A: 0.486, dist [34, 1008, 0, 0, 2275, 1910, 2007, 2766]\n",
      "Ep: 11 , 38400/60000,  L: 4.438, A: 0.486, dist [36, 1008, 0, 0, 2265, 1941, 1989, 2761]\n",
      "Ep: 11 , 51200/60000,  L: 4.294, A: 0.486, dist [45, 1044, 0, 0, 2234, 1883, 1976, 2818]\n",
      "Ep: 12 , 00000/60000,  L: 4.494, A: 0.487, dist [39, 1036, 0, 0, 2242, 1930, 1991, 2762]\n",
      "Ep: 12 , 12800/60000,  L: 4.446, A: 0.487, dist [45, 1029, 0, 0, 2242, 1915, 1995, 2774]\n",
      "Ep: 12 , 25600/60000,  L: 4.450, A: 0.486, dist [45, 1021, 0, 0, 2238, 1888, 1992, 2816]\n",
      "Ep: 12 , 38400/60000,  L: 4.296, A: 0.486, dist [47, 1017, 0, 0, 2190, 1889, 2004, 2853]\n",
      "Ep: 12 , 51200/60000,  L: 4.369, A: 0.486, dist [52, 1041, 0, 0, 2202, 1901, 1972, 2832]\n",
      "Ep: 13 , 00000/60000,  L: 4.462, A: 0.487, dist [53, 1035, 0, 0, 2204, 1933, 1978, 2797]\n",
      "Ep: 13 , 12800/60000,  L: 4.356, A: 0.487, dist [45, 1025, 0, 0, 2211, 1911, 1995, 2813]\n",
      "Ep: 13 , 25600/60000,  L: 4.376, A: 0.486, dist [48, 1040, 0, 0, 2206, 1889, 1985, 2832]\n",
      "Ep: 13 , 38400/60000,  L: 4.344, A: 0.487, dist [48, 1027, 0, 0, 2174, 1906, 1984, 2861]\n",
      "Ep: 13 , 51200/60000,  L: 4.394, A: 0.487, dist [53, 1034, 0, 0, 2194, 1912, 1984, 2823]\n",
      "Ep: 14 , 00000/60000,  L: 4.443, A: 0.487, dist [52, 1030, 0, 0, 2206, 1914, 1975, 2823]\n",
      "Ep: 14 , 12800/60000,  L: 4.401, A: 0.487, dist [51, 1031, 0, 0, 2186, 1915, 1979, 2838]\n",
      "Ep: 14 , 25600/60000,  L: 4.301, A: 0.487, dist [54, 1028, 0, 0, 2200, 1911, 1986, 2821]\n",
      "Ep: 14 , 38400/60000,  L: 4.243, A: 0.487, dist [51, 1031, 0, 0, 2156, 1890, 1982, 2890]\n",
      "Ep: 14 , 51200/60000,  L: 4.480, A: 0.487, dist [55, 1024, 0, 0, 2171, 1870, 1992, 2888]\n"
     ]
    }
   ],
   "source": [
    "model = NTree3()\n",
    "n_epochs = 15\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_dl = iter(train_loader)\n",
    "final_dist = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    train_dl = iter(train_loader)\n",
    "    \n",
    "    for i, batch in enumerate(train_dl):\n",
    "        data, labels = batch\n",
    "        bz = data.size()[0]\n",
    "        data_var, labels_var = Variable(data), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        leaf_dist, path_prob, p_nodes = model(data_var)\n",
    "        \n",
    "        loss = total_loss(path_prob, leaf_dist, labels_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%200 == 0:\n",
    "            acc, correct, total, dist = acc_calc(test_loader, model)\n",
    "            print('Ep: %d , %05d/60000,  L: %.03f, A: %.03f, dist %s' % (epoch, i*bz ,loss.data[0], acc, list(dist.long().numpy())))\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Variable containing:\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       "  0.0291  0.0282  0.0724  0.0615  0.0158  0.3144  0.0237  0.0089  0.4373  0.0088\n",
       " [torch.FloatTensor of size 32x10], Variable containing:\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       "  0.0129  0.3581  0.0380  0.2472  0.0390  0.0205  0.0090  0.0358  0.0375  0.2021\n",
       " [torch.FloatTensor of size 32x10], Variable containing:\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       "  0.0209  0.0153  0.0222  0.0105  0.3759  0.1539  0.3145  0.0210  0.0336  0.0322\n",
       " [torch.FloatTensor of size 32x10], Variable containing:\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       "  0.1913  0.0297  0.2385  0.0231  0.0443  0.0260  0.0474  0.2876  0.0387  0.0735\n",
       " [torch.FloatTensor of size 32x10]]"
      ]
     },
     "execution_count": 941,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.leaf_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc, correct, total, dist = acc_calc(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1936"
      ]
     },
     "execution_count": 926,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 927,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7586, 0, 0, 2414]"
      ]
     },
     "execution_count": 936,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dist.long().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "-2.0772\n",
       "-0.6781\n",
       "-1.6316\n",
       "-1.6203\n",
       " 0.3809\n",
       "-1.5100\n",
       "-1.1012\n",
       " 1.9916\n",
       "-1.4977\n",
       " 1.8553\n",
       "[torch.FloatTensor of size 10]"
      ]
     },
     "execution_count": 914,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.leaves[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "-2.2890\n",
       " 2.0111\n",
       "-0.4270\n",
       " 0.0372\n",
       "-1.9340\n",
       " 0.5527\n",
       "-1.2689\n",
       "-0.9730\n",
       " 1.6722\n",
       "-1.1189\n",
       "[torch.FloatTensor of size 10]"
      ]
     },
     "execution_count": 915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.leaves[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
